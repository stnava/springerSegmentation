% Encoding: UTF-8

@Manual{R-ANTsR,
  title  = {ANTsR: ANTs in R: quantification tools for biomedical images},
  author = {Brian B. Avants and Benjamin M. Kandel and Jeff T. Duda and Philip A. Cook and Nicholas J. Tustison},
  year   = {2016},
  note   = {R package version 0.3.3},
}

@Article{ZhangLiDengEtAl2015,
  author          = {Zhang, Wenlu and Li, Rongjian and Deng, Houtao and Wang, Li and Lin, Weili and Ji, Shuiwang and Shen, Dinggang},
  title           = {Deep convolutional neural networks for multi-modality isointense infant brain image segmentation.},
  journal         = {NeuroImage},
  year            = {2015},
  volume          = {108},
  pages           = {214--224},
  month           = {Mar},
  abstract        = {The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) plays an important role in studying early brain development in health and disease. In the isointense stage (approximately 6-8 months of age), WM and GM exhibit similar levels of intensity in both T1 and T2 MR images, making the tissue segmentation very challenging. Only a small number of existing methods have been designed for tissue segmentation in this isointense stage; however, they only used a single T1 or T2 images, or the combination of T1 and T2 images. In this paper, we propose to use deep convolutional neural networks (CNNs) for segmenting isointense stage brain tissues using multi-modality MR images. CNNs are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. Specifically, we used multi-modality information from T1, T2, and fractional anisotropy (FA) images as inputs and then generated the segmentation maps as outputs. The multiple intermediate layers applied convolution, pooling, normalization, and other operations to capture the highly nonlinear mappings between inputs and outputs. We compared the performance of our approach with that of the commonly used segmentation methods on a set of manually segmented isointense stage brain images. Results showed that our proposed model significantly outperformed prior methods on infant brain tissue segmentation. In addition, our results indicated that integration of multi-modality images led to significant performance improvement.},
  citation-subset = {IM},
  completed       = {2016-01-07},
  country         = {United States},
  created         = {2015-02-09},
  doi             = {10.1016/j.neuroimage.2014.12.061},
  issn            = {1095-9572},
  issn-linking    = {1053-8119},
  keywords        = {Anisotropy; Brain, anatomy & histology; Brain Mapping, methods; Gray Matter; Humans; Image Processing, Computer-Assisted, methods; Infant; Magnetic Resonance Imaging, methods; Neural Networks (Computer); White Matter; Convolutional neural networks; Deep learning; Image segmentation; Infant brain image; Multi-modality data},
  mid             = {NIHMS653703},
  nlm             = {PMC4323729},
  nlm-id          = {9215515},
  owner           = {NLM},
  pii             = {S1053-8119(14)01066-0},
  pmc             = {PMC4323729},
  pmid            = {25562829},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{ChoiJin2016,
  author       = {Choi, Hongyoon and Jin, Kyong Hwan},
  title        = {Fast and robust segmentation of the striatum using deep convolutional neural networks.},
  journal      = {Journal of neuroscience methods},
  year         = {2016},
  volume       = {274},
  pages        = {146--153},
  month        = {Dec},
  abstract     = {Automated segmentation of brain structures is an important task in structural and functional image analysis. We developed a fast and accurate method for the striatum segmentation using deep convolutional neural networks (CNN). T1 magnetic resonance (MR) images were used for our CNN-based segmentation, which require neither image feature extraction nor nonlinear transformation. We employed two serial CNN, Global and Local CNN: The Global CNN determined approximate locations of the striatum. It performed a regression of input MR images fitted to smoothed segmentation maps of the striatum. From the output volume of Global CNN, cropped MR volumes which included the striatum were extracted. The cropped MR volumes and the output volumes of Global CNN were used for inputs of Local CNN. Local CNN predicted the accurate label of all voxels. Segmentation results were compared with a widely used segmentation method, FreeSurfer. Our method showed higher Dice Similarity Coefficient (DSC) (0.893±0.017 vs. 0.786±0.015) and precision score (0.905±0.018 vs. 0.690±0.022) than FreeSurfer-based striatum segmentation (p=0.06). Our approach was also tested using another independent dataset, which showed high DSC (0.826±0.038) comparable with that of FreeSurfer. Comparison with existing method Segmentation performance of our proposed method was comparable with that of FreeSurfer. The running time of our approach was approximately three seconds. We suggested a fast and accurate deep CNN-based segmentation for small brain structures which can be widely applied to brain image analysis.},
  country      = {Netherlands},
  created      = {2016-10-25},
  doi          = {10.1016/j.jneumeth.2016.10.007},
  issn         = {1872-678X},
  issn-linking = {0165-0270},
  keywords     = {Convolutional neural network; Deep learning; MRI; Segmentation; Striatum},
  nlm-id       = {7905558},
  owner        = {NLM},
  pii          = {S0165-0270(16)30232-1},
  pmid         = {27777000},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-11-20},
}

@Article{KorfiatisKlineErickson2016,
  author    = {Korfiatis, Panagiotis and Kline, Timothy L and Erickson, Bradley J},
  title     = {Automated Segmentation of Hyperintense Regions in FLAIR MRI Using Deep Learning.},
  journal   = {Tomography : a journal for imaging research},
  year      = {2016},
  volume    = {2},
  pages     = {334--340},
  month     = {Dec},
  abstract  = {We present a deep convolutional neural network application based on autoencoders aimed at segmentation of increased signal regions in fluid-attenuated inversion recovery magnetic resonance imaging images. The convolutional autoencoders were trained on the publicly available Brain Tumor Image Segmentation Benchmark (BRATS) data set, and the accuracy was evaluated on a data set where 3 expert segmentations were available. The simultaneous truth and performance level estimation (STAPLE) algorithm was used to provide the ground truth for comparison, and Dice coefficient, Jaccard coefficient, true positive fraction, and false negative fraction were calculated. The proposed technique was within the interobserver variability with respect to Dice, Jaccard, and true positive fraction. The developed method can be used to produce automatic segmentations of tumor regions corresponding to signal-increased fluid-attenuated inversion recovery regions.},
  country   = {United States},
  created   = {2017-01-09},
  doi       = {10.18383/j.tom.2016.00166},
  issue     = {4},
  keywords  = {FLAIR; autoencoders; convolution; segmentation},
  mid       = {NIHMS839225},
  nlm-id    = {101671170},
  owner     = {NLM},
  pmc       = {PMC5215737},
  pmid      = {28066806},
  pubmodel  = {Print},
  pubstatus = {ppublish},
  revised   = {2017-01-12},
}

@Article{HavaeiDavyWarde-FarleyEtAl2017,
  author       = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre-Marc and Larochelle, Hugo},
  title        = {Brain tumor segmentation with Deep Neural Networks.},
  journal      = {Medical image analysis},
  year         = {2017},
  volume       = {35},
  pages        = {18--31},
  month        = {Jan},
  abstract     = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.},
  country      = {Netherlands},
  created      = {2016-06-16},
  doi          = {10.1016/j.media.2016.05.004},
  issn         = {1361-8423},
  issn-linking = {1361-8415},
  keywords     = {Brain tumor segmentation; Cascaded convolutional neural networks; Convolutional neural networks; Deep neural networks},
  nlm-id       = {9713490},
  owner        = {NLM},
  pii          = {S1361-8415(16)30033-0},
  pmid         = {27310171},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-11-07},
}

@Article{LiuKitschMillerEtAl2016,
  author          = {Liu, Mengyuan and Kitsch, Averi and Miller, Steven and Chau, Vann and Poskitt, Kenneth and Rousseau, Francois and Shaw, Dennis and Studholme, Colin},
  title           = {Patch-based augmentation of Expectation-Maximization for brain MRI tissue segmentation at arbitrary age after premature birth.},
  journal         = {NeuroImage},
  year            = {2016},
  volume          = {127},
  pages           = {387--408},
  month           = {Feb},
  abstract        = {Accurate automated tissue segmentation of premature neonatal magnetic resonance images is a crucial task for quantification of brain injury and its impact on early postnatal growth and later cognitive development. In such studies it is common for scans to be acquired shortly after birth or later during the hospital stay and therefore occur at arbitrary gestational ages during a period of rapid developmental change. It is important to be able to segment any of these scans with comparable accuracy. Previous work on brain tissue segmentation in premature neonates has focused on segmentation at specific ages. Here we look at solving the more general problem using adaptations of age specific atlas based methods and evaluate this using a unique manually traced database of high resolution images spanning 20 gestational weeks of development. We examine the complimentary strengths of age specific atlas-based Expectation-Maximization approaches and patch-based methods for this problem and explore the development of two new hybrid techniques, patch-based augmentation of Expectation-Maximization with weighted fusion and a spatial variability constrained patch search. The former approach seeks to combine the advantages of both atlas- and patch-based methods by learning from the performance of the two techniques across the brain anatomy at different developmental ages, while the latter technique aims to use anatomical variability maps learnt from atlas training data to locally constrain the patch-based search range. The proposed approaches were evaluated using leave-one-out cross-validation. Compared with the conventional age specific atlas-based segmentation and direct patch based segmentation, both new approaches demonstrate improved accuracy in the automated labeling of cortical gray matter, white matter, ventricles and sulcal cortical-spinal fluid regions, while maintaining comparable results in deep gray matter.},
  citation-subset = {IM},
  completed       = {2016-12-13},
  country         = {United States},
  created         = {2016-02-15},
  doi             = {10.1016/j.neuroimage.2015.12.009},
  issn            = {1095-9572},
  issn-linking    = {1053-8119},
  keywords        = {Algorithms; Brain, anatomy & histology; Female; Humans; Image Processing, Computer-Assisted, methods; Infant, Newborn; Infant, Premature; Magnetic Resonance Imaging, methods; Male; Neuroimaging, methods; Atlas-based; Expectation–Maximization; MRI; Patch-based; Premature neonates; Segmentation; Spatio-temporal},
  mid             = {NIHMS745361},
  nlm             = {PMC4755845 [Available on 02/15/17]},
  nlm-id          = {9215515},
  owner           = {NLM},
  pii             = {S1053-8119(15)01122-2},
  pmc             = {PMC4755845},
  pmid            = {26702777},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-30},
}

@Article{BroschTamAlzheimersDiseaseNeuroimaging2013,
  author          = {Brosch, Tom and Tam, Roger and Initiative for the Alzheimers Disease Neuroimaging},
  title           = {Manifold learning of brain MRIs by deep learning.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2013},
  volume          = {16},
  pages           = {633--640},
  abstract        = {Manifold learning of medical images plays a potentially important role for modeling anatomical variability within a population with pplications that include segmentation, registration, and prediction of clinical parameters. This paper describes a novel method for learning the manifold of 3D brain images that, unlike most existing manifold learning methods, does not require the manifold space to be locally linear, and does not require a predefined similarity measure or a prebuilt proximity graph. Our manifold learning method is based on deep learning, a machine learning approach that uses layered networks (called deep belief networks, or DBNs) and has received much attention recently in the computer vision field due to their success in object recognition tasks. DBNs have traditionally been too computationally expensive for application to 3D images due to the large number of trainable parameters. Our primary contributions are (1) a much more computationally efficient training method for DBNs that makes training on 3D medical images with a resolution of up to 128 x 128 x 128 practical, and (2) the demonstration that DBNs can learn a low-dimensional manifold of brain volumes that detects modes of variations that correlate to demographic and disease parameters.},
  citation-subset = {IM},
  completed       = {2014-04-03},
  country         = {Germany},
  created         = {2014-02-28},
  issue           = {Pt 2},
  keywords        = {Algorithms; Artificial Intelligence; Brain, pathology; Brain Diseases, pathology; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Imaging, Three-Dimensional, methods; Information Storage and Retrieval, methods; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmid            = {24579194},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2014-02-28},
}

@Article{XingXieYang2016,
  author          = {Xing, Fuyong and Xie, Yuanpu and Yang, Lin},
  title           = {An Automatic Learning-Based Framework for Robust Nucleus Segmentation.},
  journal         = {IEEE transactions on medical imaging},
  year            = {2016},
  volume          = {35},
  pages           = {550--566},
  month           = {Feb},
  abstract        = {Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.},
  citation-subset = {IM},
  completed       = {2016-12-13},
  country         = {United States},
  created         = {2016-04-06},
  doi             = {10.1109/TMI.2015.2481436},
  issn            = {1558-254X},
  issn-linking    = {0278-0062},
  issue           = {2},
  keywords        = {Algorithms; Brain, diagnostic imaging; Brain Neoplasms, diagnostic imaging; Breast, diagnostic imaging; Breast Neoplasms, diagnostic imaging; Cell Nucleus, pathology; Female; Humans; Image Processing, Computer-Assisted, methods; Neural Networks (Computer)},
  nlm-id          = {8310780},
  owner           = {NLM},
  pmid            = {26415167},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-30},
}

@Article{Helms2016,
  author          = {Helms, Gunther},
  title           = {Segmentation of human brain using structural MRI.},
  journal         = {Magma (New York, N.Y.)},
  year            = {2016},
  volume          = {29},
  pages           = {111--124},
  month           = {Apr},
  abstract        = {Segmentation of human brain using structural MRI is a key step of processing in imaging neuroscience. The methods have undergone a rapid development in the past two decades and are now widely available. This non-technical review aims at providing an overview and basic understanding of the most common software. Starting with the basis of structural MRI contrast in brain and imaging protocols, the concepts of voxel-based and surface-based segmentation are discussed. Special emphasis is given to the typical contrast features and morphological constraints of cortical and sub-cortical grey matter. In addition to the use for voxel-based morphometry, basic applications in quantitative MRI, cortical thickness estimations, and atrophy measurements as well as assignment of cortical regions and deep brain nuclei are briefly discussed. Finally, some fields for clinical applications are given.},
  citation-subset = {IM},
  completed       = {2016-12-30},
  country         = {Germany},
  created         = {2016-04-14},
  doi             = {10.1007/s10334-015-0518-z},
  issn            = {1352-8661},
  issn-linking    = {0968-5243},
  issue           = {2},
  keywords        = {Algorithms; Brain, anatomy & histology, diagnostic imaging; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Imaging, Three-Dimensional, methods; Machine Learning; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity; Brain; Cortical thickness; MRI; Morphometry; Segmentation},
  nlm-id          = {9310752},
  owner           = {NLM},
  pii             = {10.1007/s10334-015-0518-z},
  pmid            = {26739264},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-31},
}

@Article{KamnitsasLedigNewcombeEtAl2017,
  author       = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F J and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
  title        = {Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation.},
  journal      = {Medical image analysis},
  year         = {2017},
  volume       = {36},
  pages        = {61--78},
  month        = {Feb},
  abstract     = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
  country      = {Netherlands},
  created      = {2016-11-19},
  doi          = {10.1016/j.media.2016.10.004},
  issn         = {1361-8423},
  issn-linking = {1361-8415},
  keywords     = {3D convolutional neural network; Brain lesions; Deep learning; Fully connected CRF; Segmentation},
  nlm-id       = {9713490},
  owner        = {NLM},
  pii          = {S1361-8415(16)30183-9},
  pmid         = {27865153},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2017-01-16},
}

@Article{ZhaoWangNiuEtAl2016,
  author          = {Zhao, Guangjun and Wang, Xuchu and Niu, Yanmin and Tan, Liwen and Zhang, Shao-Xiang},
  title           = {Segmenting Brain Tissues from Chinese Visible Human Dataset by Deep-Learned Features with Stacked Autoencoder.},
  journal         = {BioMed research international},
  year            = {2016},
  volume          = {2016},
  pages           = {5284586},
  abstract        = {Cryosection brain images in Chinese Visible Human (CVH) dataset contain rich anatomical structure information of tissues because of its high resolution (e.g., 0.167 mm per pixel). Fast and accurate segmentation of these images into white matter, gray matter, and cerebrospinal fluid plays a critical role in analyzing and measuring the anatomical structures of human brain. However, most existing automated segmentation methods are designed for computed tomography or magnetic resonance imaging data, and they may not be applicable for cryosection images due to the imaging difference. In this paper, we propose a supervised learning-based CVH brain tissues segmentation method that uses stacked autoencoder (SAE) to automatically learn the deep feature representations. Specifically, our model includes two successive parts where two three-layer SAEs take image patches as input to learn the complex anatomical feature representation, and then these features are sent to Softmax classifier for inferring the labels. Experimental results validated the effectiveness of our method and showed that it outperformed four other classical brain tissue detection strategies. Furthermore, we reconstructed three-dimensional surfaces of these tissues, which show their potential in exploring the high-resolution anatomical structures of human brain.},
  citation-subset = {IM},
  completed       = {2017-01-03},
  country         = {United States},
  created         = {2016-04-08},
  doi             = {10.1155/2016/5284586},
  issn            = {2314-6141},
  keywords        = {Brain, anatomy & histology, diagnostic imaging; Humans; Image Processing, Computer-Assisted, methods; Supervised Machine Learning; Visible Human Projects},
  nlm             = {PMC4807075},
  nlm-id          = {101600173},
  owner           = {NLM},
  pmc             = {PMC4807075},
  pmid            = {27057543},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2017-01-04},
}

@Article{DolzBetrouniQuidetEtAl2016,
  author          = {Dolz, Jose and Betrouni, Nacim and Quidet, Mathilde and Kharroubi, Dris and Leroy, Henri A and Reyns, Nicolas and Massoptier, Laurent and Vermandel, Maximilien},
  title           = {Stacking denoising auto-encoders in a deep network to segment the brainstem on MRI in brain cancer patients: A clinical study.},
  journal         = {Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society},
  year            = {2016},
  volume          = {52},
  pages           = {8--18},
  month           = {Sep},
  abstract        = {Delineation of organs at risk (OARs) is a crucial step in surgical and treatment planning in brain cancer, where precise OARs volume delineation is required. However, this task is still often manually performed, which is time-consuming and prone to observer variability. To tackle these issues a deep learning approach based on stacking denoising auto-encoders has been proposed to segment the brainstem on magnetic resonance images in brain cancer context. Additionally to classical features used in machine learning to segment brain structures, two new features are suggested. Four experts participated in this study by segmenting the brainstem on 9 patients who underwent radiosurgery. Analysis of variance on shape and volume similarity metrics indicated that there were significant differences (p<0.05) between the groups of manual annotations and automatic segmentations. Experimental evaluation also showed an overlapping higher than 90% with respect to the ground truth. These results are comparable, and often higher, to those of the state of the art segmentation methods but with a considerably reduction of the segmentation time.},
  citation-subset = {IM},
  country         = {United States},
  created         = {2016-07-04},
  doi             = {10.1016/j.compmedimag.2016.03.003},
  issn            = {1879-0771},
  issn-linking    = {0895-6111},
  keywords        = {Brain cancer; Deep learning; MRI segmentation; Machine learning},
  nlm-id          = {8806104},
  owner           = {NLM},
  pii             = {S0895-6111(16)30029-5},
  pmid            = {27236370},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-07-04},
}

@Article{DengBaoDengEtAl2016,
  author       = {Deng, Yue and Bao, Feng and Deng, Xuesong and Wang, Ruiping and Kong, Youyong and Dai, Qionghai},
  title        = {Deep and Structured Robust Information Theoretic Learning for Image Analysis.},
  journal      = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
  year         = {2016},
  month        = {Jul},
  abstract     = {This paper presents a robust information theoretic (RIT) model to reduce the uncertainties, i.e. missing and noisy labels, in general discriminative data representation tasks. The fundamental pursuit of our model is to simultaneously learn a transformation function and a discriminative classifier that maximize the mutual information of data and their labels in the latent space. In this general paradigm, we respectively discuss three types of the RIT implementations with linear subspace embedding, deep transformation and structured sparse learning. In practice, the RIT and deep RIT are exploited to solve the image categorization task whose performances will be verified on various benchmark datasets. The structured sparse RIT is further applied to a medical image analysis task for brain MRI segmentation that allows group-level feature selections on the brain tissues.},
  country      = {United States},
  created      = {2016-07-08},
  doi          = {10.1109/TIP.2016.2588330},
  issn         = {1941-0042},
  issn-linking = {1057-7149},
  nlm-id       = {9886191},
  owner        = {NLM},
  pmid         = {27392359},
  pubmodel     = {Print-Electronic},
  pubstatus    = {aheadofprint},
  revised      = {2016-07-08},
}

@Article{Arganda-CarrerasTuragaBergerEtAl2015,
  author       = {Arganda-Carreras, Ignacio and Turaga, Srinivas C and Berger, Daniel R and Cireşan, Dan and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, Jürgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D and Bas, Erhan and Uzunbas, Mustafa G and Cardona, Albert and Schindelin, Johannes and Seung, H Sebastian},
  title        = {Crowdsourcing the creation of image segmentation algorithms for connectomics.},
  journal      = {Frontiers in neuroanatomy},
  year         = {2015},
  volume       = {9},
  pages        = {142},
  abstract     = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic (EM) images of the brain. Participants submitted boundary maps predicted for a test set of images, and were scored based on their agreement with a consensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This "deep learning" approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from cooperation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test dataset. Retrospective evaluation of the challenge scoring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
  completed    = {2015-11-23},
  country      = {Switzerland},
  created      = {2015-11-23},
  doi          = {10.3389/fnana.2015.00142},
  issn-linking = {1662-5129},
  keywords     = {connectomics; electron microscopy; image segmentation; machine learning; reconstruction},
  nlm          = {PMC4633678},
  nlm-id       = {101477943},
  owner        = {NLM},
  pmc          = {PMC4633678},
  pmid         = {26594156},
  pubmodel     = {Electronic-eCollection},
  pubstatus    = {epublish},
  revised      = {2015-12-14},
}

@Article{StoneWildeTaylorEtAl2016,
  author       = {Stone, James R and Wilde, Elisabeth A and Taylor, Brian A and Tate, David F and Levin, Harvey and Bigler, Erin D and Scheibel, Randall S and Newsome, Mary R and Mayer, Andrew R and Abildskov, Tracy and Black, Garrett M and Lennon, Michael J and York, Gerald E and Agarwal, Rajan and DeVillasante, Jorge and Ritter, John L and Walker, Peter B and Ahlers, Stephen T and Tustison, Nicholas J},
  title        = {Supervised learning technique for the automated identification of white matter hyperintensities in traumatic brain injury.},
  journal      = {Brain injury},
  year         = {2016},
  volume       = {30},
  pages        = {1458--1468},
  abstract     = {White matter hyperintensities (WMHs) are foci of abnormal signal intensity in white matter regions seen with magnetic resonance imaging (MRI). WMHs are associated with normal ageing and have shown prognostic value in neurological conditions such as traumatic brain injury (TBI). The impracticality of manually quantifying these lesions limits their clinical utility and motivates the utilization of machine learning techniques for automated segmentation workflows. This study develops a concatenated random forest framework with image features for segmenting WMHs in a TBI cohort. The framework is built upon the Advanced Normalization Tools (ANTs) and ANTsR toolkits. MR (3D FLAIR, T2- and T1-weighted) images from 24 service members and veterans scanned in the Chronic Effects of Neurotrauma Consortium's (CENC) observational study were acquired. Manual annotations were employed for both training and evaluation using a leave-one-out strategy. Performance measures include sensitivity, positive predictive value, [Formula: see text] score and relative volume difference. Final average results were: sensitivity = 0.68 ± 0.38, positive predictive value = 0.51 ± 0.40, [Formula: see text] = 0.52 ± 0.36, relative volume difference = 43 ± 26%. In addition, three lesion size ranges are selected to illustrate the variation in performance with lesion size. Paired with correlative outcome data, supervised learning methods may allow for identification of imaging features predictive of diagnosis and prognosis in individual TBI patients.},
  country      = {England},
  created      = {2016-11-11},
  doi          = {10.1080/02699052.2016.1222080},
  issn         = {1362-301X},
  issn-linking = {0269-9052},
  issue        = {12},
  keywords     = {Neuroimaging; TBI; brain imaging; deep learning; machine learning; magnetic resonance imaging; random forest decision tree},
  nlm-id       = {8710358},
  owner        = {NLM},
  pmid         = {27834541},
  pubmodel     = {Print},
  pubstatus    = {ppublish},
  revised      = {2016-11-12},
}

@Article{ChengNiChouEtAl2016,
  author          = {Cheng, Jie-Zhi and Ni, Dong and Chou, Yi-Hong and Qin, Jing and Tiu, Chui-Mei and Chang, Yeun-Chung and Huang, Chiun-Sheng and Shen, Dinggang and Chen, Chung-Ming},
  title           = {Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in US Images and Pulmonary Nodules in CT Scans.},
  journal         = {Scientific reports},
  year            = {2016},
  volume          = {6},
  pages           = {24454},
  month           = {Apr},
  abstract        = {This paper performs a comprehensive study on the deep-learning-based computer-aided diagnosis (CADx) for the differential diagnosis of benign and malignant nodules/lesions by avoiding the potential errors caused by inaccurate image processing results (e.g., boundary segmentation), as well as the classification bias resulting from a less robust feature set, as involved in most conventional CADx algorithms. Specifically, the stacked denoising auto-encoder (SDAE) is exploited on the two CADx applications for the differentiation of breast ultrasound lesions and lung CT nodules. The SDAE architecture is well equipped with the automatic feature exploration mechanism and noise tolerance advantage, and hence may be suitable to deal with the intrinsically noisy property of medical image data from various imaging modalities. To show the outperformance of SDAE-based CADx over the conventional scheme, two latest conventional CADx algorithms are implemented for comparison. 10 times of 10-fold cross-validations are conducted to illustrate the efficacy of the SDAE-based CADx algorithm. The experimental results show the significant performance boost by the SDAE-based CADx algorithm over the two conventional methods, suggesting that deep learning techniques can potentially change the design paradigm of the CADx systems without the need of explicit design and selection of problem-oriented features.},
  citation-subset = {IM},
  country         = {England},
  created         = {2016-04-15},
  doi             = {10.1038/srep24454},
  issn            = {2045-2322},
  issn-linking    = {2045-2322},
  nlm             = {PMC4832199},
  nlm-id          = {101563288},
  owner           = {NLM},
  pii             = {srep24454},
  pmc             = {PMC4832199},
  pmid            = {27079888},
  pubmodel        = {Electronic},
  pubstatus       = {epublish},
  revised         = {2016-04-20},
}

@Article{HabesErusToledoEtAl2016,
  author          = {Habes, Mohamad and Erus, Guray and Toledo, Jon B and Zhang, Tianhao and Bryan, Nick and Launer, Lenore J and Rosseel, Yves and Janowitz, Deborah and Doshi, Jimit and Van der Auwera, Sandra and von Sarnowski, Bettina and Hegenscheid, Katrin and Hosten, Norbert and Homuth, Georg and Völzke, Henry and Schminke, Ulf and Hoffmann, Wolfgang and Grabe, Hans J and Davatzikos, Christos},
  title           = {White matter hyperintensities and imaging patterns of brain ageing in the general population.},
  journal         = {Brain : a journal of neurology},
  year            = {2016},
  volume          = {139},
  pages           = {1164--1179},
  month           = {Apr},
  abstract        = {White matter hyperintensities are associated with increased risk of dementia and cognitive decline. The current study investigates the relationship between white matter hyperintensities burden and patterns of brain atrophy associated with brain ageing and Alzheimer's disease in a large populatison-based sample (n = 2367) encompassing a wide age range (20-90 years), from the Study of Health in Pomerania. We quantified white matter hyperintensities using automated segmentation and summarized atrophy patterns using machine learning methods resulting in two indices: the SPARE-BA index (capturing age-related brain atrophy), and the SPARE-AD index (previously developed to capture patterns of atrophy found in patients with Alzheimer's disease). A characteristic pattern of age-related accumulation of white matter hyperintensities in both periventricular and deep white matter areas was found. Individuals with high white matter hyperintensities burden showed significantly (P < 0.0001) lower SPARE-BA and higher SPARE-AD values compared to those with low white matter hyperintensities burden, indicating that the former had more patterns of atrophy in brain regions typically affected by ageing and Alzheimer's disease dementia. To investigate a possibly causal role of white matter hyperintensities, structural equation modelling was used to quantify the effect of Framingham cardiovascular disease risk score and white matter hyperintensities burden on SPARE-BA, revealing a statistically significant (P < 0.0001) causal relationship between them. Structural equation modelling showed that the age effect on SPARE-BA was mediated by white matter hyperintensities and cardiovascular risk score each explaining 10.4% and 21.6% of the variance, respectively. The direct age effect explained 70.2% of the SPARE-BA variance. Only white matter hyperintensities significantly mediated the age effect on SPARE-AD explaining 32.8% of the variance. The direct age effect explained 66.0% of the SPARE-AD variance. Multivariable regression showed significant relationship between white matter hyperintensities volume and hypertension (P = 0.001), diabetes mellitus (P = 0.023), smoking (P = 0.002) and education level (P = 0.003). The only significant association with cognitive tests was with the immediate recall of the California verbal and learning memory test. No significant association was present with the APOE genotype. These results support the hypothesis that white matter hyperintensities contribute to patterns of brain atrophy found in beyond-normal brain ageing in the general population. White matter hyperintensities also contribute to brain atrophy patterns in regions related to Alzheimer's disease dementia, in agreement with their known additive role to the likelihood of dementia. Preventive strategies reducing the odds to develop cardiovascular disease and white matter hyperintensities could decrease the incidence or delay the onset of dementia.},
  citation-subset = {AIM, IM},
  completed       = {2016-08-18},
  country         = {England},
  created         = {2016-03-25},
  doi             = {10.1093/brain/aww008},
  issn            = {1460-2156},
  issn-linking    = {0006-8950},
  issue           = {Pt 4},
  keywords        = {Adult; Aged; Aged, 80 and over; Aging, pathology; Alzheimer Disease, diagnosis, epidemiology; Brain, pathology; Cognition Disorders, diagnosis, epidemiology; Cohort Studies; Dementia, diagnosis, epidemiology; Female; Germany, epidemiology; Humans; Magnetic Resonance Imaging, trends; Male; Middle Aged; Poland, epidemiology; Population Surveillance, methods; Risk Factors; White Matter, pathology; Young Adult; Alzheimer’s disease; brain ageing; cardiovascular disease; mild cognitive impairment; white matter hyperintensities},
  nlm             = {PMC5006227 [Available on 04/01/17]},
  nlm-id          = {0372537},
  owner           = {NLM},
  pii             = {aww008},
  pmc             = {PMC5006227},
  pmid            = {26912649},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-15},
}

@Article{GuoWuCommanderEtAl2014,
  author          = {Guo, Yanrong and Wu, Guorong and Commander, Leah A and Szary, Stephanie and Jewells, Valerie and Lin, Weili and Shent, Dinggang},
  title           = {Segmenting hippocampus from infant brains by sparse patch matching with deep-learned features.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2014},
  volume          = {17},
  pages           = {308--315},
  abstract        = {Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1- and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the interpatch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods.},
  citation-subset = {IM},
  completed       = {2015-01-08},
  country         = {Germany},
  created         = {2014-12-08},
  issue           = {Pt 2},
  keywords        = {Aging, pathology, physiology; Artificial Intelligence; Hippocampus, anatomy & histology, growth & development; Humans; Image Interpretation, Computer-Assisted, methods; Infant; Infant, Newborn; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  mid             = {NIHMS691875},
  nlm             = {PMC4445142},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmc             = {PMC4445142},
  pmid            = {25485393},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-10-25},
}

@Article{WangDasPlutaEtAl2010,
  author          = {Wang, Hongzhi and Das, Sandhitsu and Pluta, John and Craige, Caryne and Altinay, Murat and Avants, Brian and Weiner, Michael and Mueller, Susanne and Yushkevich, Paul},
  title           = {Standing on the shoulders of giants: improving medical image segmentation via bias correction.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2010},
  volume          = {13},
  pages           = {105--112},
  abstract        = {We propose a simple strategy to improve automatic medical image segmentation. The key idea is that without deep understanding of a segmentation method, we can still improve its performance by directly calibrating its results with respect to manual segmentation. We formulate the calibration process as a bias correction problem, which is addressed by machine learning using training data. We apply this methodology on three segmentation problems/methods and show significant improvements for all of them.},
  citation-subset = {IM},
  completed       = {2010-11-15},
  country         = {Germany},
  created         = {2010-09-30},
  issue           = {Pt 3},
  keywords        = {Algorithms; Artifacts; Brain, anatomy & histology; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  mid             = {NIHMS279851},
  nlm             = {PMC3095022},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmc             = {PMC3095022},
  pmid            = {20879389},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-12-08},
}

@Article{AdamsWilson2011,
  author          = {Adams, Christina M and Wilson, Timothy D},
  title           = {Virtual cerebral ventricular system: an MR-based three-dimensional computer model.},
  journal         = {Anatomical sciences education},
  year            = {2011},
  volume          = {4},
  pages           = {340--347},
  abstract        = {The inherent spatial complexity of the human cerebral ventricular system, coupled with its deep position within the brain, poses a problem for conceptualizing its anatomy. Cadaveric dissection, while considered the gold standard of anatomical learning, may be inadequate for learning the anatomy of the cerebral ventricular system; even with intricate dissection, ventricular structures remain difficult to observe. Three-dimensional (3D) computer reconstruction of the ventricular system offers a solution to this problem. This study aims to create an accurate 3D computer reconstruction of the ventricular system with surrounding structures, including the brain and cerebellum, using commercially available 3D rendering software. Magnetic resonance imaging (MRI) scans of a male cadaver were segmented using both semiautomatic and manual tools. Segmentation involves separating voxels of different grayscale values to highlight specific neural structures. User controls enable adding or removing of structures, altering their opacity, and making cross-sectional slices through the model to highlight inner structures. Complex physiologic concepts, such as the flow of cerebrospinal fluid, are also shown using the 3D model of the ventricular system through a video animation. The model can be projected stereoscopically, to increase depth perception and to emphasize spatial relationships between anatomical structures. This model is suited for both self-directed learning and classroom teaching of the 3D anatomical structure and spatial orientation of the ventricles, their connections, and their relation to adjacent neural and skeletal structures.},
  citation-subset = {IM},
  completed       = {2012-03-02},
  country         = {United States},
  created         = {2011-11-08},
  doi             = {10.1002/ase.256},
  issn            = {1935-9780},
  issn-linking    = {1935-9772},
  issue           = {6},
  keywords        = {Aged, 80 and over; Anatomy, education; Cadaver; Cerebral Ventricles, anatomy & histology; Computer Graphics; Computer Simulation; Computer-Assisted Instruction; Humans; Imaging, Three-Dimensional; Learning; Magnetic Resonance Imaging; Male; Models, Anatomic; Software; User-Computer Interface},
  nlm-id          = {101392205},
  owner           = {NLM},
  pmid            = {21976457},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2011-11-08},
}

@Article{McDevittRoweBradyEtAl2014,
  author          = {McDevitt, Elizabeth A and Rowe, Kelly M and Brady, Mark and Duggan, Katherine A and Mednick, Sara C},
  title           = {The benefit of offline sleep and wake for novel object recognition.},
  journal         = {Experimental brain research},
  year            = {2014},
  volume          = {232},
  pages           = {1487--1496},
  month           = {May},
  abstract        = {How do we segment and recognize novel objects? When explicit cues from motion and color are available, object boundary detection is relatively easy. However, under conditions of deep camouflage, in which objects share the same image cues as their background, the visual system must reassign new functional roles to existing image statistics in order to group continuities for detection and segmentation of object boundaries. This bootstrapped learning process is stimulus dependent and requires extensive task-specific training. Using a between-subject design, we tested participants on their ability to segment and recognize novel objects after a consolidation period of sleep or wake. We found a specific role for rapid eye movement (REM, n = 43) sleep in context-invariant novel object learning, and that REM sleep as well as a period of active wake (AW, n = 35) increased segmentation of context-specific object learning compared to a period of quiet wake (QW, n = 38; p = .007 and p = .017, respectively). Performance in the non-REM nap group (n = 32) was not different from the other groups. The REM sleep enhancement effect was especially robust for the top performing quartile of subjects, or "super learners" (p = .037). Together, these results suggest that the construction and generalization of novel representations through bootstrapped learning may benefit from REM sleep, and more specific object learning may also benefit from AW. We discuss these results in the context of shared electrophysiological and neurochemical features of AW and REM sleep, which are distinct from QW and non-REM sleep.},
  citation-subset = {IM},
  completed       = {2014-12-17},
  country         = {Germany},
  created         = {2014-05-06},
  doi             = {10.1007/s00221-014-3830-3},
  issn            = {1432-1106},
  issn-linking    = {0014-4819},
  issue           = {5},
  keywords        = {Adolescent; Adult; Analysis of Variance; Female; Humans; Male; Pattern Recognition, Visual, physiology; Photic Stimulation; Polysomnography; Recognition (Psychology), physiology; Sleep, physiology; Time Factors; Wakefulness, physiology; Young Adult},
  nlm-id          = {0043312},
  owner           = {NLM},
  pmid            = {24504196},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2014-05-06},
}

@Book{vonEcon,
  title     = {Die Cytoarchitektonik der Hirnrinde des erwachsenen Menschen},
  publisher = {Wien: Springer Verlag},
  year      = {1925},
  author    = {Constantin Freiherr von Economo and Georg N Koskinas},
}

@Article{MendrikVinckenKuijfEtAl2015,
  author          = {Mendrik, Adriënne M and Vincken, Koen L and Kuijf, Hugo J and Breeuwer, Marcel and Bouvy, Willem H and de Bresser, Jeroen and Alansary, Amir and de Bruijne, Marleen and Carass, Aaron and El-Baz, Ayman and Jog, Amod and Katyal, Ranveer and Khan, Ali R and van der Lijn, Fedde and Mahmood, Qaiser and Mukherjee, Ryan and van Opbroek, Annegreet and Paneri, Sahil and Pereira, Sérgio and Persson, Mikael and Rajchl, Martin and Sarikaya, Duygu and Smedby, Örjan and Silva, Carlos A and Vrooman, Henri A and Vyas, Saurabh and Wang, Chunliang and Zhao, Liang and Biessels, Geert Jan and Viergever, Max A},
  title           = {MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans.},
  journal         = {Computational intelligence and neuroscience},
  year            = {2015},
  volume          = {2015},
  pages           = {813696},
  abstract        = {Many methods have been proposed for tissue segmentation in brain MRI scans. The multitude of methods proposed complicates the choice of one method above others. We have therefore established the MRBrainS online evaluation framework for evaluating (semi)automatic algorithms that segment gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) on 3T brain MRI scans of elderly subjects (65-80 y). Participants apply their algorithms to the provided data, after which their results are evaluated and ranked. Full manual segmentations of GM, WM, and CSF are available for all scans and used as the reference standard. Five datasets are provided for training and fifteen for testing. The evaluated methods are ranked based on their overall performance to segment GM, WM, and CSF and evaluated using three evaluation metrics (Dice, H95, and AVD) and the results are published on the MRBrainS13 website. We present the results of eleven segmentation algorithms that participated in the MRBrainS13 challenge workshop at MICCAI, where the framework was launched, and three commonly used freeware packages: FreeSurfer, FSL, and SPM. The MRBrainS evaluation framework provides an objective and direct comparison of all evaluated algorithms and can aid in selecting the best performing method for the segmentation goal at hand.},
  citation-subset = {IM},
  completed       = {2016-09-06},
  country         = {United States},
  created         = {2016-01-13},
  doi             = {10.1155/2015/813696},
  issn            = {1687-5273},
  keywords        = {Aged; Aged, 80 and over; Algorithms; Brain, anatomy & histology, physiology; Cerebrospinal Fluid, physiology; Databases, Factual; Female; Gray Matter, anatomy & histology, physiology; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging, methods; Male; Online Systems; Reference Standards; Reproducibility of Results; Software; White Matter, anatomy & histology, physiology},
  nlm             = {PMC4680055},
  nlm-id          = {101279357},
  owner           = {NLM},
  pmc             = {PMC4680055},
  pmid            = {26759553},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{MenzeJakabBauerEtAl2015,
  author          = {Menze, Bjoern H and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-André and Arbel, Tal and Avants, Brian B and Ayache, Nicholas and Buendia, Patricia and Collins, D Louis and Cordier, Nicolas and Corso, Jason J and Criminisi, Antonio and Das, Tilak and Delingette, Hervé and Demiralp, Çağatay and Durst, Christopher R and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M and Jena, Raj and John, Nigel M and Konukoglu, Ender and Lashkari, Danial and Mariz, José Antonió and Meier, Raphael and Pereira, Sérgio and Precup, Doina and Price, Stephen J and Raviv, Tammy Riklin and Reza, Syed M S and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A and Sousa, Nuno and Subbanna, Nagesh K and Szekely, Gabor and Taylor, Thomas J and Thomas, Owen M and Tustison, Nicholas J and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
  title           = {The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS).},
  journal         = {IEEE transactions on medical imaging},
  year            = {2015},
  volume          = {34},
  pages           = {1993--2024},
  month           = {Oct},
  abstract        = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  citation-subset = {IM},
  completed       = {2016-07-06},
  country         = {United States},
  created         = {2015-10-07},
  doi             = {10.1109/TMI.2014.2377694},
  issn            = {1558-254X},
  issn-linking    = {0278-0062},
  issue           = {10},
  keywords        = {Algorithms; Benchmarking; Glioma, pathology; Humans; Magnetic Resonance Imaging, methods, standards; Neuroimaging, methods, standards},
  mid             = {NIHMS775317},
  nlm             = {PMC4833122 [Available on 10/01/16]},
  nlm-id          = {8310780},
  owner           = {NLM},
  pmc             = {PMC4833122},
  pmid            = {25494501},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{WangSuhDasEtAl2013,
  author          = {Wang, Hongzhi and Suh, Jung W and Das, Sandhitsu R and Pluta, John B and Craige, Caryne and Yushkevich, Paul A},
  title           = {Multi-Atlas Segmentation with Joint Label Fusion.},
  journal         = {IEEE transactions on pattern analysis and machine intelligence},
  year            = {2013},
  volume          = {35},
  pages           = {611--623},
  month           = {Mar},
  abstract        = {Multi-atlas segmentation is an effective approach for automatically labeling objects of interest in biomedical images. In this approach, multiple expert-segmented example images, called atlases, are registered to a target image, and deformed atlas segmentations are combined using label fusion. Among the proposed label fusion strategies, weighted voting with spatially varying weight distributions derived from atlas-target intensity similarity have been particularly successful. However, one limitation of these strategies is that the weights are computed independently for each atlas, without taking into account the fact that different atlases may produce similar label errors. To address this limitation, we propose a new solution for the label fusion problem in which weighted voting is formulated in terms of minimizing the total expectation of labeling error and in which pairwise dependency between atlases is explicitly modeled as the joint probability of two atlases making a segmentation error at a voxel. This probability is approximated using intensity similarity between a pair of atlases and the target image in the neighborhood of each voxel. We validate our method in two medical image segmentation problems: hippocampus segmentation and hippocampus subfield segmentation in magnetic resonance (MR) images. For both problems, we show consistent and significant improvement over label fusion strategies that assign atlas weights independently.},
  citation-subset = {IM},
  completed       = {2016-05-17},
  country         = {United States},
  created         = {2015-09-10},
  doi             = {10.1109/TPAMI.2012.143},
  issn            = {1939-3539},
  issn-linking    = {0098-5589},
  issue           = {3},
  keywords        = {Algorithms; Databases, Factual; Hippocampus, anatomy & histology; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging},
  mid             = {NIHMS410560},
  nlm             = {PMC3864549},
  nlm-id          = {9885960},
  owner           = {NLM},
  pmc             = {PMC3864549},
  pmid            = {22732662},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-25},
}

@Article{Hinton2014,
  author          = {Hinton, Geoffrey},
  title           = {Where do features come from?},
  journal         = {Cognitive science},
  year            = {2014},
  volume          = {38},
  pages           = {1078--1101},
  month           = {Aug},
  abstract        = {It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.},
  citation-subset = {IM},
  completed       = {2015-04-23},
  country         = {United States},
  created         = {2014-08-19},
  doi             = {10.1111/cogs.12049},
  issn            = {1551-6709},
  issn-linking    = {0364-0213},
  issue           = {6},
  keywords        = {Artificial Intelligence; Computer Simulation; Humans; Learning; Models, Neurological; Neural Networks (Computer); Backpropagation; Boltzmann machines; Contrastive divergence; Deep learning; Distributed representations; Learning features; Learning graphical models; Variational learning},
  nlm-id          = {7708195},
  owner           = {NLM},
  pmid            = {23800216},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2014-08-19},
}

@Article{LeCunBengioHinton2015,
  author          = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title           = {Deep learning.},
  journal         = {Nature},
  year            = {2015},
  volume          = {521},
  pages           = {436--444},
  month           = {May},
  abstract        = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  citation-subset = {IM},
  completed       = {2015-06-29},
  country         = {England},
  created         = {2015-05-28},
  doi             = {10.1038/nature14539},
  issn            = {1476-4687},
  issn-linking    = {0028-0836},
  issue           = {7553},
  keywords        = {Algorithms; Artificial Intelligence, trends; Computers; Language; Neural Networks (Computer)},
  nlm-id          = {0410462},
  owner           = {NLM},
  pii             = {nature14539},
  pmid            = {26017442},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2015-05-28},
}

@Article{Hinton2010,
  author          = {Hinton, Geoffrey E},
  title           = {Learning to represent visual input.},
  journal         = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
  year            = {2010},
  volume          = {365},
  pages           = {177--184},
  month           = {Jan},
  abstract        = {One of the central problems in computational neuroscience is to understand how the object-recognition pathway of the cortex learns a deep hierarchy of nonlinear feature detectors. Recent progress in machine learning shows that it is possible to learn deep hierarchies without requiring any labelled data. The feature detectors are learned one layer at a time and the goal of the learning procedure is to form a good generative model of images, not to predict the class of each image. The learning procedure only requires the pairwise correlations between the activations of neuron-like processing units in adjacent layers. The original version of the learning procedure is derived from a quadratic 'energy' function but it can be extended to allow third-order, multiplicative interactions in which neurons gate the pairwise interactions between other neurons. A technique for factoring the third-order interactions leads to a learning module that again has a simple learning rule based on pairwise correlations. This module looks remarkably like modules that have been proposed by both biologists trying to explain the responses of neurons and engineers trying to create systems that can recognize objects.},
  citation-subset = {IM},
  completed       = {2010-03-22},
  country         = {England},
  created         = {2009-12-17},
  doi             = {10.1098/rstb.2009.0200},
  issn            = {1471-2970},
  issn-linking    = {0962-8436},
  issue           = {1537},
  keywords        = {Computer Simulation; Humans; Learning, physiology; Models, Neurological; Neural Networks (Computer); Visual Pathways, physiology},
  nlm             = {PMC2842706},
  nlm-id          = {7503623},
  owner           = {NLM},
  pii             = {365/1537/177},
  pmc             = {PMC2842706},
  pmid            = {20008395},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  references      = {33},
  revised         = {2014-12-04},
}

@Article{SalakhutdinovHinton2012,
  author       = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  title        = {An efficient learning procedure for deep Boltzmann machines.},
  journal      = {Neural computation},
  year         = {2012},
  volume       = {24},
  pages        = {1967--2006},
  month        = {Aug},
  abstract     = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.},
  completed    = {2012-10-17},
  country      = {United States},
  created      = {2012-06-13},
  doi          = {10.1162/NECO_a_00311},
  issn         = {1530-888X},
  issn-linking = {0899-7667},
  issue        = {8},
  nlm-id       = {9426182},
  owner        = {NLM},
  pmid         = {22509963},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2012-06-13},
}

@Article{HintonSalakhutdinov2011,
  author          = {Hinton, Geoffrey and Salakhutdinov, Ruslan},
  title           = {Discovering binary codes for documents by learning deep generative models.},
  journal         = {Topics in cognitive science},
  year            = {2011},
  volume          = {3},
  pages           = {74--91},
  month           = {Jan},
  abstract        = {We describe a deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top-down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF-IDF we achieve higher accuracy than TF-IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.},
  citation-subset = {IM},
  completed       = {2016-03-31},
  country         = {United States},
  created         = {2014-08-28},
  doi             = {10.1111/j.1756-8765.2010.01109.x},
  issn            = {1756-8765},
  issn-linking    = {1756-8757},
  issue           = {1},
  keywords        = {Artificial Intelligence; Documentation, methods; Information Storage and Retrieval, methods; Models, Theoretical; Semantics; Auto-encoders; Binary codes; Deep learning; Document retrieval; Restricted Boltzmann machines; Semantic hashing},
  nlm-id          = {101506764},
  owner           = {NLM},
  pmid            = {25164175},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2014-08-28},
}

@Article{Hinton2007,
  author          = {Hinton, Geoffrey E},
  title           = {Learning multiple layers of representation.},
  journal         = {Trends in cognitive sciences},
  year            = {2007},
  volume          = {11},
  pages           = {428--434},
  month           = {Oct},
  abstract        = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
  citation-subset = {IM},
  completed       = {2008-02-01},
  country         = {England},
  created         = {2007-10-23},
  doi             = {10.1016/j.tics.2007.09.004},
  issn            = {1364-6613},
  issn-linking    = {1364-6613},
  issue           = {10},
  keywords        = {Brain, physiology; Humans; Learning, physiology; Models, Psychological; Nerve Net, physiology},
  nlm-id          = {9708669},
  owner           = {NLM},
  pii             = {S1364-6613(07)00217-3},
  pmid            = {17921042},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  references      = {44},
  revised         = {2007-10-23},
}

@Article{HintonOsinderoTeh2006,
  author          = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  title           = {A fast learning algorithm for deep belief nets.},
  journal         = {Neural computation},
  year            = {2006},
  volume          = {18},
  pages           = {1527--1554},
  month           = {Jul},
  abstract        = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  citation-subset = {IM},
  completed       = {2006-07-26},
  country         = {United States},
  created         = {2006-06-12},
  doi             = {10.1162/neco.2006.18.7.1527},
  issn            = {0899-7667},
  issn-linking    = {0899-7667},
  issue           = {7},
  keywords        = {Algorithms; Animals; Humans; Learning, physiology; Neural Networks (Computer); Neurons, physiology},
  nlm-id          = {9426182},
  owner           = {NLM},
  pmid            = {16764513},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2006-11-15},
}

@Article{SutskeverHinton2008,
  author          = {Sutskever, Ilya and Hinton, Geoffrey E},
  title           = {Deep, narrow sigmoid belief networks are universal approximators.},
  journal         = {Neural computation},
  year            = {2008},
  volume          = {20},
  pages           = {2629--2636},
  month           = {Nov},
  abstract        = {In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.},
  citation-subset = {IM},
  completed       = {2008-12-12},
  country         = {United States},
  created         = {2008-09-29},
  doi             = {10.1162/neco.2008.12-07-661},
  issn            = {0899-7667},
  issn-linking    = {0899-7667},
  issue           = {11},
  keywords        = {Algorithms; Humans; Learning; Neural Networks (Computer); Nonlinear Dynamics},
  nlm-id          = {9426182},
  owner           = {NLM},
  pmid            = {18533819},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2008-09-29},
}

@Article{LeRouxBengio2008,
  author          = {Le Roux, Nicolas and Bengio, Yoshua},
  title           = {Representational power of restricted boltzmann machines and deep belief networks.},
  journal         = {Neural computation},
  year            = {2008},
  volume          = {20},
  pages           = {1631--1649},
  month           = {Jun},
  abstract        = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.},
  citation-subset = {IM},
  completed       = {2008-07-10},
  country         = {United States},
  created         = {2008-05-23},
  doi             = {10.1162/neco.2008.04-07-510},
  issn            = {0899-7667},
  issn-linking    = {0899-7667},
  issue           = {6},
  keywords        = {Algorithms; Animals; Computer Simulation; Humans; Learning, physiology; Models, Statistical; Neural Networks (Computer); Signal Processing, Computer-Assisted},
  nlm-id          = {9426182},
  owner           = {NLM},
  pmid            = {18254699},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2008-05-23},
}

@Misc{HeZhangRenEtAl2015,
  author   = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title    = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In: arXiv:abs/1502.01852 [cs.CV},
  year     = {2015},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94 % top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26 % relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66 % [33]). To our knowledge, our result is the first1 to surpass the reported human-level performance (5.1%, [26]) on this dataset.},
}

@Article{WuSchusterChenEtAl2016,
  author      = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
  title       = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  year        = {2016},
  abstract    = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.},
  date        = {2016-09-26},
  eprint      = {1609.08144v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1609.08144v2:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
}

@Article{Goldsborough2016,
  author      = {Peter Goldsborough},
  title       = {A Tour of TensorFlow},
  year        = {2016},
  abstract    = {Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.},
  date        = {2016-10-01},
  eprint      = {1610.01178v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1610.01178v1:PDF},
  keywords    = {cs.LG},
}

@Article{SchoenickClarkTafjordEtAl2016,
  author      = {Carissa Schoenick and Peter Clark and Oyvind Tafjord and Peter Turney and Oren Etzioni},
  title       = {Moving Beyond the Turing Test with the Allen AI Science Challenge},
  year        = {2016},
  abstract    = {Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in the game of GO), it's become increasingly important to assess: how close are AI systems to human-level intelligence? This paper describes the Allen AI Science Challenge---an approach towards that goal which led to a unique Kaggle Competition, its results, the lessons learned, and our next steps.},
  date        = {2016-04-14},
  eprint      = {1604.04315v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1604.04315v2:PDF},
  keywords    = {cs.AI},
}

@Article{SantoroBartunovBotvinickEtAl2016,
  author  = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  title   = {One-shot learning with memory-augmented neural networks},
  journal = {arXiv preprint arXiv:1605.06065},
  year    = {2016},
}

@Article{MordvintsevOlahTyka2015,
  author  = {Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
  title   = {Inceptionism: Going deeper into neural networks},
  journal = {Google Research Blog. Retrieved June},
  year    = {2015},
  volume  = {20},
  pages   = {14},
}

@InProceedings{Le2013,
  author       = {Le, Quoc V},
  title        = {Building high-level features using large scale unsupervised learning},
  booktitle    = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  year         = {2013},
  pages        = {8595--8598},
  organization = {IEEE},
}

@Article{LeRanzatoMongaEtAl2011,
  author      = {Quoc V. Le and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Greg S. Corrado and Jeff Dean and Andrew Y. Ng},
  title       = {Building high-level features using large scale unsupervised learning},
  journal     = {International Conference in Machine Learning},
  year        = {2012},
  abstract    = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative im- provement over the previous state-of-the-art.},
  date        = {2011-12-29},
  eprint      = {1112.6209v5},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1112.6209v5:PDF},
  keywords    = {cs.LG},
}

@Article{MoeskopsViergeverMendrikEtAl2016,
  author       = {Moeskops, Pim and Viergever, Max A and Mendrik, Adrienne M and de Vries, Linda S and Benders, Manon J N L and Isgum, Ivana},
  title        = {Automatic Segmentation of MR Brain Images With a Convolutional Neural Network.},
  journal      = {IEEE transactions on medical imaging},
  year         = {2016},
  volume       = {35},
  pages        = {1252--1261},
  month        = {May},
  abstract     = {Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T2-weighted images of preterm infants acquired at 40 weeks PMA, axial T1-weighted images of ageing adults acquired at an average age of 70 years, and T1-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.},
  country      = {United States},
  created      = {2016-04-05},
  doi          = {10.1109/TMI.2016.2548501},
  issn         = {1558-254X},
  issn-linking = {0278-0062},
  issue        = {5},
  nlm-id       = {8310780},
  owner        = {NLM},
  pmid         = {27046893},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-12-22},
}

@InProceedings{NohHongHan2015,
  author    = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  title     = {Learning deconvolution network for semantic segmentation},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year      = {2015},
  pages     = {1520--1528},
}

@InProceedings{ZeilerKrishnanTaylorEtAl2010,
  author       = {Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
  title        = {Deconvolutional networks},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  year         = {2010},
  pages        = {2528--2535},
  organization = {IEEE},
}

@InProceedings{ZeilerFergus2014,
  author       = {Zeiler, Matthew D and Fergus, Rob},
  title        = {Visualizing and understanding convolutional networks},
  booktitle    = {European conference on computer vision},
  year         = {2014},
  pages        = {818--833},
  organization = {Springer},
}

@Article{BroschTangYooEtAl2016,
  author       = {Brosch, Tom and Tang, Lisa Y W and Youngjin Yoo and Li, David K B and Traboulsee, Anthony and Tam, Roger},
  title        = {Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation.},
  journal      = {IEEE transactions on medical imaging},
  year         = {2016},
  volume       = {35},
  pages        = {1229--1239},
  month        = {May},
  abstract     = {We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.},
  country      = {United States},
  created      = {2016-02-17},
  doi          = {10.1109/TMI.2016.2528821},
  issn         = {1558-254X},
  issn-linking = {0278-0062},
  issue        = {5},
  nlm-id       = {8310780},
  owner        = {NLM},
  pmid         = {26886978},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-12-22},
}

@Article{TustisonCookKleinEtAl2014,
  author          = {Tustison, Nicholas J and Cook, Philip A and Klein, Arno and Song, Gang and Das, Sandhitsu R and Duda, Jeffrey T and Kandel, Benjamin M and van Strien, Niels and Stone, James R and Gee, James C and Avants, Brian B},
  title           = {Large-scale evaluation of ANTs and FreeSurfer cortical thickness measurements.},
  journal         = {NeuroImage},
  year            = {2014},
  volume          = {99},
  pages           = {166--179},
  month           = {Oct},
  abstract        = {Many studies of the human brain have explored the relationship between cortical thickness and cognition, phenotype, or disease. Due to the subjectivity and time requirements in manual measurement of cortical thickness, scientists have relied on robust software tools for automation which facilitate the testing and refinement of neuroscientific hypotheses. The most widely used tool for cortical thickness studies is the publicly available, surface-based FreeSurfer package. Critical to the adoption of such tools is a demonstration of their reproducibility, validity, and the documentation of specific implementations that are robust across large, diverse imaging datasets. To this end, we have developed the automated, volume-based Advanced Normalization Tools (ANTs) cortical thickness pipeline comprising well-vetted components such as SyGN (multivariate template construction), SyN (image registration), N4 (bias correction), Atropos (n-tissue segmentation), and DiReCT (cortical thickness estimation). In this work, we have conducted the largest evaluation of automated cortical thickness measures in publicly available data, comparing FreeSurfer and ANTs measures computed on 1205 images from four open data sets (IXI, MMRR, NKI, and OASIS), with parcellation based on the recently proposed Desikan-Killiany-Tourville (DKT) cortical labeling protocol. We found good scan-rescan repeatability with both FreeSurfer and ANTs measures. Given that such assessments of precision do not necessarily reflect accuracy or an ability to make statistical inferences, we further tested the neurobiological validity of these approaches by evaluating thickness-based prediction of age and gender. ANTs is shown to have a higher predictive performance than FreeSurfer for both of these measures. In promotion of open science, we make all of our scripts, data, and results publicly available which complements the use of open image data sets and the open source availability of the proposed ANTs cortical thickness pipeline.},
  citation-subset = {IM},
  completed       = {2015-04-13},
  country         = {United States},
  created         = {2014-07-26},
  doi             = {10.1016/j.neuroimage.2014.05.044},
  issn            = {1095-9572},
  issn-linking    = {1053-8119},
  keywords        = {Adolescent; Adult; Aged; Aged, 80 and over; Aging, physiology; Algorithms; Cerebral Cortex, anatomy & histology, growth & development; Child; Child, Preschool; Databases, Factual; Female; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging; Male; Middle Aged; Reproducibility of Results; Sex Characteristics; Software; Young Adult; Advanced Normalization Tools; Age prediction; Gender prediction; MRI; Open science; Scientific reproducibility},
  nlm-id          = {9215515},
  owner           = {NLM},
  pii             = {S1053-8119(14)00409-1},
  pmid            = {24879923},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2014-07-26},
}

@Article{CordierDelingetteAyache2016,
  author          = {Cordier, Nicolas and Delingette, Herve and Ayache, Nicholas},
  title           = {A Patch-Based Approach for the Segmentation of Pathologies: Application to Glioma Labelling.},
  journal         = {IEEE transactions on medical imaging},
  year            = {2016},
  volume          = {35},
  pages           = {1066--1076},
  month           = {Apr},
  abstract        = {In this paper, we describe a novel and generic approach to address fully-automatic segmentation of brain tumors by using multi-atlas patch-based voting techniques. In addition to avoiding the local search window assumption, the conventional patch-based framework is enhanced through several simple procedures: an improvement of the training dataset in terms of both label purity and intensity statistics, augmented features to implicitly guide the nearest-neighbor-search, multi-scale patches, invariance to cube isometries, stratification of the votes with respect to cases and labels. A probabilistic model automatically delineates regions of interest enclosing high-probability tumor volumes, which allows the algorithm to achieve highly competitive running time despite minimal processing power and resources. This method was evaluated on Multimodal Brain Tumor Image Segmentation challenge datasets. State-of-the-art results are achieved, with a limited learning stage thus restricting the risk of overfit. Moreover, segmentation smoothness does not involve any post-processing.},
  citation-subset = {IM},
  completed       = {2016-12-30},
  country         = {United States},
  created         = {2016-04-06},
  doi             = {10.1109/TMI.2015.2508150},
  issn            = {1558-254X},
  issn-linking    = {0278-0062},
  issue           = {4},
  keywords        = {Algorithms; Brain, diagnostic imaging; Brain Neoplasms, diagnostic imaging; Glioma, diagnostic imaging; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging},
  nlm-id          = {8310780},
  owner           = {NLM},
  pmid            = {26685225},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-31},
}

@Article{JanowczykMadabhushi2016,
  author    = {Janowczyk, Andrew and Madabhushi, Anant},
  title     = {Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases.},
  journal   = {Journal of pathology informatics},
  year      = {2016},
  volume    = {7},
  pages     = {29},
  abstract  = {Deep learning (DL) is a representation learning approach ideally suited for image analysis challenges in digital pathology (DP). The variety of image analysis tasks in the context of DP includes detection and counting (e.g., mitotic events), segmentation (e.g., nuclei), and tissue classification (e.g., cancerous vs. non-cancerous). Unfortunately, issues with slide preparation, variations in staining and scanning across sites, and vendor platforms, as well as biological variance, such as the presentation of different grades of disease, make these image analysis tasks particularly challenging. Traditional approaches, wherein domain-specific cues are manually identified and developed into task-specific "handcrafted" features, can require extensive tuning to accommodate these variances. However, DL takes a more domain agnostic approach combining both feature discovery and implementation to maximally discriminate between the classes of interest. While DL approaches have performed well in a few DP related image analysis tasks, such as detection and tissue classification, the currently available open source tools and tutorials do not provide guidance on challenges such as (a) selecting appropriate magnification, (b) managing errors in annotations in the training (or learning) dataset, and (c) identifying a suitable training set containing information rich exemplars. These foundational concepts, which are needed to successfully translate the DL paradigm to DP tasks, are non-trivial for (i) DL experts with minimal digital histology experience, and (ii) DP and image processing experts with minimal DL experience, to derive on their own, thus meriting a dedicated tutorial. This paper investigates these concepts through seven unique DP tasks as use cases to elucidate techniques needed to produce comparable, and in many cases, superior to results from the state-of-the-art hand-crafted feature-based classification approaches. Specifically, in this tutorial on DL for DP image analysis, we show how an open source framework (Caffe), with a singular network architecture, can be used to address: (a) nuclei segmentation (F-score of 0.83 across 12,000 nuclei), (b) epithelium segmentation (F-score of 0.84 across 1735 regions), (c) tubule segmentation (F-score of 0.83 from 795 tubules), (d) lymphocyte detection (F-score of 0.90 across 3064 lymphocytes), (e) mitosis detection (F-score of 0.53 across 550 mitotic events), (f) invasive ductal carcinoma detection (F-score of 0.7648 on 50 k testing patches), and (g) lymphoma classification (classification accuracy of 0.97 across 374 images). This paper represents the largest comprehensive study of DL approaches in DP to date, with over 1200 DP images used during evaluation. The supplemental online material that accompanies this paper consists of step-by-step instructions for the usage of the supplied source code, trained models, and input data.},
  completed = {2016-08-26},
  country   = {India},
  created   = {2016-08-26},
  doi       = {10.4103/2153-3539.186902},
  issn      = {2229-5089},
  keywords  = {Classification; deep learning; detection; digital histology; machine learning; segmentation},
  nlm       = {PMC4977982},
  nlm-id    = {101528849},
  owner     = {NLM},
  pii       = {JPI-7-29},
  pmc       = {PMC4977982},
  pmid      = {27563488},
  pubmodel  = {Electronic-eCollection},
  pubstatus = {epublish},
  revised   = {2016-08-28},
}

@Article{WangYushkevich2013,
  author       = {Wang, Hongzhi and Yushkevich, Paul A},
  title        = {Multi-atlas segmentation with joint label fusion and corrective learning-an open source implementation.},
  journal      = {Frontiers in neuroinformatics},
  year         = {2013},
  volume       = {7},
  pages        = {27},
  abstract     = {Label fusion based multi-atlas segmentation has proven to be one of the most competitive techniques for medical image segmentation. This technique transfers segmentations from expert-labeled images, called atlases, to a novel image using deformable image registration. Errors produced by label transfer are further reduced by label fusion that combines the results produced by all atlases into a consensus solution. Among the proposed label fusion strategies, weighted voting with spatially varying weight distributions derived from atlas-target intensity similarity is a simple and highly effective label fusion technique. However, one limitation of most weighted voting methods is that the weights are computed independently for each atlas, without taking into account the fact that different atlases may produce similar label errors. To address this problem, we recently developed the joint label fusion technique and the corrective learning technique, which won the first place of the 2012 MICCAI Multi-Atlas Labeling Challenge and was one of the top performers in 2013 MICCAI Segmentation: Algorithms, Theory and Applications (SATA) challenge. To make our techniques more accessible to the scientific research community, we describe an Insight-Toolkit based open source implementation of our label fusion methods. Our implementation extends our methods to work with multi-modality imaging data and is more suitable for segmentation problems with multiple labels. We demonstrate the usage of our tools through applying them to the 2012 MICCAI Multi-Atlas Labeling Challenge brain image dataset and the 2013 SATA challenge canine leg image dataset. We report the best results on these two datasets so far.},
  completed    = {2013-12-09},
  country      = {Switzerland},
  created      = {2013-12-09},
  doi          = {10.3389/fninf.2013.00027},
  issn-linking = {1662-5196},
  keywords     = {Insight-Toolkit; corrective learning; joint label fusion; multi-atlas label fusion; open source implementation},
  nlm          = {PMC3837555},
  nlm-id       = {101477957},
  owner        = {NLM},
  pmc          = {PMC3837555},
  pmid         = {24319427},
  pubmodel     = {Electronic-eCollection},
  pubstatus    = {epublish},
  revised      = {2016-10-25},
}

@Article{LuXuRodrigueEtAl2011,
  author          = {Lu, Hanzhang and Xu, Feng and Rodrigue, Karen M and Kennedy, Kristen M and Cheng, Yamei and Flicker, Blair and Hebrank, Andrew C and Uh, Jinsoo and Park, Denise C},
  title           = {Alterations in cerebral metabolic rate and blood supply across the adult lifespan.},
  journal         = {Cerebral cortex (New York, N.Y. : 1991)},
  year            = {2011},
  volume          = {21},
  pages           = {1426--1434},
  month           = {Jun},
  __markedentry   = {[bavants:]},
  abstract        = {With age, the brain undergoes comprehensive changes in its function and physiology. Cerebral metabolism and blood supply are among the key physiologic processes supporting the daily function of the brain and may play an important role in age-related cognitive decline. Using MRI, it is now possible to make quantitative assessment of these parameters in a noninvasive manner. In the present study, we concurrently measured cerebral metabolic rate of oxygen (CMRO(2)), cerebral blood flow (CBF), and venous blood oxygenation in a well-characterized healthy adult cohort from 20 to 89 years old (N = 232). Our data showed that CMRO(2) increased significantly with age, while CBF decreased with age. This combination of higher demand and diminished supply resulted in a reduction of venous blood oxygenation with age. Regional CBF was also determined, and it was found that the spatial pattern of CBF decline was heterogeneous across the brain with prefrontal cortex, insular cortex, and caudate being the most affected regions. Aside from the resting state parameters, the blood vessels' ability to dilate, measured by cerebrovascular reactivity to 5% CO(2) inhalation, was assessed and was reduced with age, the extent of which was more prominent than that of the resting state CBF.},
  chemicals       = {Oxygen},
  citation-subset = {IM},
  completed       = {2011-09-13},
  country         = {United States},
  created         = {2011-05-20},
  doi             = {10.1093/cercor/bhq224},
  issn            = {1460-2199},
  issn-linking    = {1047-3211},
  issue           = {6},
  keywords        = {Adult; Age Factors; Aged; Aged, 80 and over; Cerebral Cortex, blood supply, metabolism; Cerebrovascular Circulation, physiology; Energy Metabolism, physiology; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging, methods; Male; Middle Aged; Oxygen, blood; Oxygen Consumption, physiology; Young Adult},
  nlm             = {PMC3097991},
  nlm-id          = {9110718},
  owner           = {NLM},
  pii             = {bhq224},
  pmc             = {PMC3097991},
  pmid            = {21051551},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{TustisonShrinidhiWintermarkEtAl2015,
  author          = {Tustison, Nicholas J and Shrinidhi, K L and Wintermark, Max and Durst, Christopher R and Kandel, Benjamin M and Gee, James C and Grossman, Murray C and Avants, Brian B},
  title           = {Optimal Symmetric Multimodal Templates and Concatenated Random Forests for Supervised Brain Tumor Segmentation (Simplified) with ANTsR.},
  journal         = {Neuroinformatics},
  year            = {2015},
  volume          = {13},
  pages           = {209--225},
  month           = {Apr},
  __markedentry   = {[bavants:6]},
  abstract        = {Segmenting and quantifying gliomas from MRI is an important task for diagnosis, planning intervention, and for tracking tumor changes over time. However, this task is complicated by the lack of prior knowledge concerning tumor location, spatial extent, shape, possible displacement of normal tissue, and intensity signature. To accommodate such complications, we introduce a framework for supervised segmentation based on multiple modality intensity, geometry, and asymmetry feature sets. These features drive a supervised whole-brain and tumor segmentation approach based on random forest-derived probabilities. The asymmetry-related features (based on optimal symmetric multimodal templates) demonstrate excellent discriminative properties within this framework. We also gain performance by generating probability maps from random forest models and using these maps for a refining Markov random field regularized probabilistic segmentation. This strategy allows us to interface the supervised learning capabilities of the random forest model with regularized probabilistic segmentation using the recently developed ANTsR package--a comprehensive statistical and visualization interface between the popular Advanced Normalization Tools (ANTs) and the R statistical project. The reported algorithmic framework was the top-performing entry in the MICCAI 2013 Multimodal Brain Tumor Segmentation challenge. The challenge data were widely varying consisting of both high-grade and low-grade glioma tumor four-modality MRI from five different institutions. Average Dice overlap measures for the final algorithmic assessment were 0.87, 0.78, and 0.74 for "complete", "core", and "enhanced" tumor components, respectively.},
  citation-subset = {IM},
  completed       = {2016-02-17},
  country         = {United States},
  created         = {2015-05-29},
  doi             = {10.1007/s12021-014-9245-2},
  issn            = {1559-0089},
  issn-linking    = {1539-2791},
  issue           = {2},
  keywords        = {Algorithms; Brain Neoplasms, pathology; Humans; Image Interpretation, Computer-Assisted; Models, Theoretical; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity},
  nlm-id          = {101142069},
  owner           = {NLM},
  pmid            = {25433513},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2015-05-29},
}

@Comment{jabref-meta: databaseType:bibtex;}
