% Encoding: UTF-8

@Manual{R-ANTsR,
  title  = {ANTsR: ANTs in R: quantification tools for biomedical images},
  author = {Brian B. Avants and Benjamin M. Kandel and Jeff T. Duda and Philip A. Cook and Nicholas J. Tustison},
  year   = {2016},
  note   = {R package version 0.3.3},
}

@Article{ZhangLiDengEtAl2015,
  author          = {Zhang, Wenlu and Li, Rongjian and Deng, Houtao and Wang, Li and Lin, Weili and Ji, Shuiwang and Shen, Dinggang},
  title           = {Deep convolutional neural networks for multi-modality isointense infant brain image segmentation.},
  journal         = {NeuroImage},
  year            = {2015},
  volume          = {108},
  pages           = {214--224},
  month           = {Mar},
  abstract        = {The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) plays an important role in studying early brain development in health and disease. In the isointense stage (approximately 6-8 months of age), WM and GM exhibit similar levels of intensity in both T1 and T2 MR images, making the tissue segmentation very challenging. Only a small number of existing methods have been designed for tissue segmentation in this isointense stage; however, they only used a single T1 or T2 images, or the combination of T1 and T2 images. In this paper, we propose to use deep convolutional neural networks (CNNs) for segmenting isointense stage brain tissues using multi-modality MR images. CNNs are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. Specifically, we used multi-modality information from T1, T2, and fractional anisotropy (FA) images as inputs and then generated the segmentation maps as outputs. The multiple intermediate layers applied convolution, pooling, normalization, and other operations to capture the highly nonlinear mappings between inputs and outputs. We compared the performance of our approach with that of the commonly used segmentation methods on a set of manually segmented isointense stage brain images. Results showed that our proposed model significantly outperformed prior methods on infant brain tissue segmentation. In addition, our results indicated that integration of multi-modality images led to significant performance improvement.},
  citation-subset = {IM},
  completed       = {2016-01-07},
  country         = {United States},
  created         = {2015-02-09},
  doi             = {10.1016/j.neuroimage.2014.12.061},
  issn            = {1095-9572},
  issn-linking    = {1053-8119},
  keywords        = {Anisotropy; Brain, anatomy & histology; Brain Mapping, methods; Gray Matter; Humans; Image Processing, Computer-Assisted, methods; Infant; Magnetic Resonance Imaging, methods; Neural Networks (Computer); White Matter; Convolutional neural networks; Deep learning; Image segmentation; Infant brain image; Multi-modality data},
  mid             = {NIHMS653703},
  nlm             = {PMC4323729},
  nlm-id          = {9215515},
  owner           = {NLM},
  pii             = {S1053-8119(14)01066-0},
  pmc             = {PMC4323729},
  pmid            = {25562829},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{ChoiJin2016,
  author       = {Choi, Hongyoon and Jin, Kyong Hwan},
  title        = {Fast and robust segmentation of the striatum using deep convolutional neural networks.},
  journal      = {Journal of neuroscience methods},
  year         = {2016},
  volume       = {274},
  pages        = {146--153},
  month        = {Dec},
  abstract     = {Automated segmentation of brain structures is an important task in structural and functional image analysis. We developed a fast and accurate method for the striatum segmentation using deep convolutional neural networks (CNN). T1 magnetic resonance (MR) images were used for our CNN-based segmentation, which require neither image feature extraction nor nonlinear transformation. We employed two serial CNN, Global and Local CNN: The Global CNN determined approximate locations of the striatum. It performed a regression of input MR images fitted to smoothed segmentation maps of the striatum. From the output volume of Global CNN, cropped MR volumes which included the striatum were extracted. The cropped MR volumes and the output volumes of Global CNN were used for inputs of Local CNN. Local CNN predicted the accurate label of all voxels. Segmentation results were compared with a widely used segmentation method, FreeSurfer. Our method showed higher Dice Similarity Coefficient (DSC) (0.893±0.017 vs. 0.786±0.015) and precision score (0.905±0.018 vs. 0.690±0.022) than FreeSurfer-based striatum segmentation (p=0.06). Our approach was also tested using another independent dataset, which showed high DSC (0.826±0.038) comparable with that of FreeSurfer. Comparison with existing method Segmentation performance of our proposed method was comparable with that of FreeSurfer. The running time of our approach was approximately three seconds. We suggested a fast and accurate deep CNN-based segmentation for small brain structures which can be widely applied to brain image analysis.},
  country      = {Netherlands},
  created      = {2016-10-25},
  doi          = {10.1016/j.jneumeth.2016.10.007},
  issn         = {1872-678X},
  issn-linking = {0165-0270},
  keywords     = {Convolutional neural network; Deep learning; MRI; Segmentation; Striatum},
  nlm-id       = {7905558},
  owner        = {NLM},
  pii          = {S0165-0270(16)30232-1},
  pmid         = {27777000},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-11-20},
}

@Article{KorfiatisKlineErickson2016,
  author    = {Korfiatis, Panagiotis and Kline, Timothy L and Erickson, Bradley J},
  title     = {Automated Segmentation of Hyperintense Regions in FLAIR MRI Using Deep Learning.},
  journal   = {Tomography : a journal for imaging research},
  year      = {2016},
  volume    = {2},
  pages     = {334--340},
  month     = {Dec},
  abstract  = {We present a deep convolutional neural network application based on autoencoders aimed at segmentation of increased signal regions in fluid-attenuated inversion recovery magnetic resonance imaging images. The convolutional autoencoders were trained on the publicly available Brain Tumor Image Segmentation Benchmark (BRATS) data set, and the accuracy was evaluated on a data set where 3 expert segmentations were available. The simultaneous truth and performance level estimation (STAPLE) algorithm was used to provide the ground truth for comparison, and Dice coefficient, Jaccard coefficient, true positive fraction, and false negative fraction were calculated. The proposed technique was within the interobserver variability with respect to Dice, Jaccard, and true positive fraction. The developed method can be used to produce automatic segmentations of tumor regions corresponding to signal-increased fluid-attenuated inversion recovery regions.},
  country   = {United States},
  created   = {2017-01-09},
  doi       = {10.18383/j.tom.2016.00166},
  issue     = {4},
  keywords  = {FLAIR; autoencoders; convolution; segmentation},
  mid       = {NIHMS839225},
  nlm-id    = {101671170},
  owner     = {NLM},
  pmc       = {PMC5215737},
  pmid      = {28066806},
  pubmodel  = {Print},
  pubstatus = {ppublish},
  revised   = {2017-01-12},
}

@Article{HavaeiDavyWarde-FarleyEtAl2017,
  author       = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre-Marc and Larochelle, Hugo},
  title        = {Brain tumor segmentation with Deep Neural Networks.},
  journal      = {Medical image analysis},
  year         = {2017},
  volume       = {35},
  pages        = {18--31},
  month        = {Jan},
  abstract     = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.},
  country      = {Netherlands},
  created      = {2016-06-16},
  doi          = {10.1016/j.media.2016.05.004},
  issn         = {1361-8423},
  issn-linking = {1361-8415},
  keywords     = {Brain tumor segmentation; Cascaded convolutional neural networks; Convolutional neural networks; Deep neural networks},
  nlm-id       = {9713490},
  owner        = {NLM},
  pii          = {S1361-8415(16)30033-0},
  pmid         = {27310171},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2016-11-07},
}

@Article{LiuKitschMillerEtAl2016,
  author          = {Liu, Mengyuan and Kitsch, Averi and Miller, Steven and Chau, Vann and Poskitt, Kenneth and Rousseau, Francois and Shaw, Dennis and Studholme, Colin},
  title           = {Patch-based augmentation of Expectation-Maximization for brain MRI tissue segmentation at arbitrary age after premature birth.},
  journal         = {NeuroImage},
  year            = {2016},
  volume          = {127},
  pages           = {387--408},
  month           = {Feb},
  abstract        = {Accurate automated tissue segmentation of premature neonatal magnetic resonance images is a crucial task for quantification of brain injury and its impact on early postnatal growth and later cognitive development. In such studies it is common for scans to be acquired shortly after birth or later during the hospital stay and therefore occur at arbitrary gestational ages during a period of rapid developmental change. It is important to be able to segment any of these scans with comparable accuracy. Previous work on brain tissue segmentation in premature neonates has focused on segmentation at specific ages. Here we look at solving the more general problem using adaptations of age specific atlas based methods and evaluate this using a unique manually traced database of high resolution images spanning 20 gestational weeks of development. We examine the complimentary strengths of age specific atlas-based Expectation-Maximization approaches and patch-based methods for this problem and explore the development of two new hybrid techniques, patch-based augmentation of Expectation-Maximization with weighted fusion and a spatial variability constrained patch search. The former approach seeks to combine the advantages of both atlas- and patch-based methods by learning from the performance of the two techniques across the brain anatomy at different developmental ages, while the latter technique aims to use anatomical variability maps learnt from atlas training data to locally constrain the patch-based search range. The proposed approaches were evaluated using leave-one-out cross-validation. Compared with the conventional age specific atlas-based segmentation and direct patch based segmentation, both new approaches demonstrate improved accuracy in the automated labeling of cortical gray matter, white matter, ventricles and sulcal cortical-spinal fluid regions, while maintaining comparable results in deep gray matter.},
  citation-subset = {IM},
  completed       = {2016-12-13},
  country         = {United States},
  created         = {2016-02-15},
  doi             = {10.1016/j.neuroimage.2015.12.009},
  issn            = {1095-9572},
  issn-linking    = {1053-8119},
  keywords        = {Algorithms; Brain, anatomy & histology; Female; Humans; Image Processing, Computer-Assisted, methods; Infant, Newborn; Infant, Premature; Magnetic Resonance Imaging, methods; Male; Neuroimaging, methods; Atlas-based; Expectation–Maximization; MRI; Patch-based; Premature neonates; Segmentation; Spatio-temporal},
  mid             = {NIHMS745361},
  nlm             = {PMC4755845 [Available on 02/15/17]},
  nlm-id          = {9215515},
  owner           = {NLM},
  pii             = {S1053-8119(15)01122-2},
  pmc             = {PMC4755845},
  pmid            = {26702777},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-30},
}

@Article{BroschTamAlzheimersDiseaseNeuroimaging2013,
  author          = {Brosch, Tom and Tam, Roger and Initiative for the Alzheimers Disease Neuroimaging},
  title           = {Manifold learning of brain MRIs by deep learning.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2013},
  volume          = {16},
  pages           = {633--640},
  abstract        = {Manifold learning of medical images plays a potentially important role for modeling anatomical variability within a population with pplications that include segmentation, registration, and prediction of clinical parameters. This paper describes a novel method for learning the manifold of 3D brain images that, unlike most existing manifold learning methods, does not require the manifold space to be locally linear, and does not require a predefined similarity measure or a prebuilt proximity graph. Our manifold learning method is based on deep learning, a machine learning approach that uses layered networks (called deep belief networks, or DBNs) and has received much attention recently in the computer vision field due to their success in object recognition tasks. DBNs have traditionally been too computationally expensive for application to 3D images due to the large number of trainable parameters. Our primary contributions are (1) a much more computationally efficient training method for DBNs that makes training on 3D medical images with a resolution of up to 128 x 128 x 128 practical, and (2) the demonstration that DBNs can learn a low-dimensional manifold of brain volumes that detects modes of variations that correlate to demographic and disease parameters.},
  citation-subset = {IM},
  completed       = {2014-04-03},
  country         = {Germany},
  created         = {2014-02-28},
  issue           = {Pt 2},
  keywords        = {Algorithms; Artificial Intelligence; Brain, pathology; Brain Diseases, pathology; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Imaging, Three-Dimensional, methods; Information Storage and Retrieval, methods; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmid            = {24579194},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2014-02-28},
}

@Article{XingXieYang2016,
  author          = {Xing, Fuyong and Xie, Yuanpu and Yang, Lin},
  title           = {An Automatic Learning-Based Framework for Robust Nucleus Segmentation.},
  journal         = {IEEE transactions on medical imaging},
  year            = {2016},
  volume          = {35},
  pages           = {550--566},
  month           = {Feb},
  abstract        = {Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.},
  citation-subset = {IM},
  completed       = {2016-12-13},
  country         = {United States},
  created         = {2016-04-06},
  doi             = {10.1109/TMI.2015.2481436},
  issn            = {1558-254X},
  issn-linking    = {0278-0062},
  issue           = {2},
  keywords        = {Algorithms; Brain, diagnostic imaging; Brain Neoplasms, diagnostic imaging; Breast, diagnostic imaging; Breast Neoplasms, diagnostic imaging; Cell Nucleus, pathology; Female; Humans; Image Processing, Computer-Assisted, methods; Neural Networks (Computer)},
  nlm-id          = {8310780},
  owner           = {NLM},
  pmid            = {26415167},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-30},
}

@Article{Helms2016,
  author          = {Helms, Gunther},
  title           = {Segmentation of human brain using structural MRI.},
  journal         = {Magma (New York, N.Y.)},
  year            = {2016},
  volume          = {29},
  pages           = {111--124},
  month           = {Apr},
  abstract        = {Segmentation of human brain using structural MRI is a key step of processing in imaging neuroscience. The methods have undergone a rapid development in the past two decades and are now widely available. This non-technical review aims at providing an overview and basic understanding of the most common software. Starting with the basis of structural MRI contrast in brain and imaging protocols, the concepts of voxel-based and surface-based segmentation are discussed. Special emphasis is given to the typical contrast features and morphological constraints of cortical and sub-cortical grey matter. In addition to the use for voxel-based morphometry, basic applications in quantitative MRI, cortical thickness estimations, and atrophy measurements as well as assignment of cortical regions and deep brain nuclei are briefly discussed. Finally, some fields for clinical applications are given.},
  citation-subset = {IM},
  completed       = {2016-12-30},
  country         = {Germany},
  created         = {2016-04-14},
  doi             = {10.1007/s10334-015-0518-z},
  issn            = {1352-8661},
  issn-linking    = {0968-5243},
  issue           = {2},
  keywords        = {Algorithms; Brain, anatomy & histology, diagnostic imaging; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Imaging, Three-Dimensional, methods; Machine Learning; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity; Brain; Cortical thickness; MRI; Morphometry; Segmentation},
  nlm-id          = {9310752},
  owner           = {NLM},
  pii             = {10.1007/s10334-015-0518-z},
  pmid            = {26739264},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-31},
}

@Article{KamnitsasLedigNewcombeEtAl2017,
  author       = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F J and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
  title        = {Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation.},
  journal      = {Medical image analysis},
  year         = {2017},
  volume       = {36},
  pages        = {61--78},
  month        = {Feb},
  abstract     = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
  country      = {Netherlands},
  created      = {2016-11-19},
  doi          = {10.1016/j.media.2016.10.004},
  issn         = {1361-8423},
  issn-linking = {1361-8415},
  keywords     = {3D convolutional neural network; Brain lesions; Deep learning; Fully connected CRF; Segmentation},
  nlm-id       = {9713490},
  owner        = {NLM},
  pii          = {S1361-8415(16)30183-9},
  pmid         = {27865153},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2017-01-16},
}

@Article{ZhaoWangNiuEtAl2016,
  author          = {Zhao, Guangjun and Wang, Xuchu and Niu, Yanmin and Tan, Liwen and Zhang, Shao-Xiang},
  title           = {Segmenting Brain Tissues from Chinese Visible Human Dataset by Deep-Learned Features with Stacked Autoencoder.},
  journal         = {BioMed research international},
  year            = {2016},
  volume          = {2016},
  pages           = {5284586},
  abstract        = {Cryosection brain images in Chinese Visible Human (CVH) dataset contain rich anatomical structure information of tissues because of its high resolution (e.g., 0.167 mm per pixel). Fast and accurate segmentation of these images into white matter, gray matter, and cerebrospinal fluid plays a critical role in analyzing and measuring the anatomical structures of human brain. However, most existing automated segmentation methods are designed for computed tomography or magnetic resonance imaging data, and they may not be applicable for cryosection images due to the imaging difference. In this paper, we propose a supervised learning-based CVH brain tissues segmentation method that uses stacked autoencoder (SAE) to automatically learn the deep feature representations. Specifically, our model includes two successive parts where two three-layer SAEs take image patches as input to learn the complex anatomical feature representation, and then these features are sent to Softmax classifier for inferring the labels. Experimental results validated the effectiveness of our method and showed that it outperformed four other classical brain tissue detection strategies. Furthermore, we reconstructed three-dimensional surfaces of these tissues, which show their potential in exploring the high-resolution anatomical structures of human brain.},
  citation-subset = {IM},
  completed       = {2017-01-03},
  country         = {United States},
  created         = {2016-04-08},
  doi             = {10.1155/2016/5284586},
  issn            = {2314-6141},
  keywords        = {Brain, anatomy & histology, diagnostic imaging; Humans; Image Processing, Computer-Assisted, methods; Supervised Machine Learning; Visible Human Projects},
  nlm             = {PMC4807075},
  nlm-id          = {101600173},
  owner           = {NLM},
  pmc             = {PMC4807075},
  pmid            = {27057543},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2017-01-04},
}

@Article{DolzBetrouniQuidetEtAl2016,
  author          = {Dolz, Jose and Betrouni, Nacim and Quidet, Mathilde and Kharroubi, Dris and Leroy, Henri A and Reyns, Nicolas and Massoptier, Laurent and Vermandel, Maximilien},
  title           = {Stacking denoising auto-encoders in a deep network to segment the brainstem on MRI in brain cancer patients: A clinical study.},
  journal         = {Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society},
  year            = {2016},
  volume          = {52},
  pages           = {8--18},
  month           = {Sep},
  abstract        = {Delineation of organs at risk (OARs) is a crucial step in surgical and treatment planning in brain cancer, where precise OARs volume delineation is required. However, this task is still often manually performed, which is time-consuming and prone to observer variability. To tackle these issues a deep learning approach based on stacking denoising auto-encoders has been proposed to segment the brainstem on magnetic resonance images in brain cancer context. Additionally to classical features used in machine learning to segment brain structures, two new features are suggested. Four experts participated in this study by segmenting the brainstem on 9 patients who underwent radiosurgery. Analysis of variance on shape and volume similarity metrics indicated that there were significant differences (p<0.05) between the groups of manual annotations and automatic segmentations. Experimental evaluation also showed an overlapping higher than 90% with respect to the ground truth. These results are comparable, and often higher, to those of the state of the art segmentation methods but with a considerably reduction of the segmentation time.},
  citation-subset = {IM},
  country         = {United States},
  created         = {2016-07-04},
  doi             = {10.1016/j.compmedimag.2016.03.003},
  issn            = {1879-0771},
  issn-linking    = {0895-6111},
  keywords        = {Brain cancer; Deep learning; MRI segmentation; Machine learning},
  nlm-id          = {8806104},
  owner           = {NLM},
  pii             = {S0895-6111(16)30029-5},
  pmid            = {27236370},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-07-04},
}

@Article{DengBaoDengEtAl2016,
  author       = {Deng, Yue and Bao, Feng and Deng, Xuesong and Wang, Ruiping and Kong, Youyong and Dai, Qionghai},
  title        = {Deep and Structured Robust Information Theoretic Learning for Image Analysis.},
  journal      = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
  year         = {2016},
  month        = {Jul},
  abstract     = {This paper presents a robust information theoretic (RIT) model to reduce the uncertainties, i.e. missing and noisy labels, in general discriminative data representation tasks. The fundamental pursuit of our model is to simultaneously learn a transformation function and a discriminative classifier that maximize the mutual information of data and their labels in the latent space. In this general paradigm, we respectively discuss three types of the RIT implementations with linear subspace embedding, deep transformation and structured sparse learning. In practice, the RIT and deep RIT are exploited to solve the image categorization task whose performances will be verified on various benchmark datasets. The structured sparse RIT is further applied to a medical image analysis task for brain MRI segmentation that allows group-level feature selections on the brain tissues.},
  country      = {United States},
  created      = {2016-07-08},
  doi          = {10.1109/TIP.2016.2588330},
  issn         = {1941-0042},
  issn-linking = {1057-7149},
  nlm-id       = {9886191},
  owner        = {NLM},
  pmid         = {27392359},
  pubmodel     = {Print-Electronic},
  pubstatus    = {aheadofprint},
  revised      = {2016-07-08},
}

@Article{Arganda-CarrerasTuragaBergerEtAl2015,
  author       = {Arganda-Carreras, Ignacio and Turaga, Srinivas C and Berger, Daniel R and Cireşan, Dan and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, Jürgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D and Bas, Erhan and Uzunbas, Mustafa G and Cardona, Albert and Schindelin, Johannes and Seung, H Sebastian},
  title        = {Crowdsourcing the creation of image segmentation algorithms for connectomics.},
  journal      = {Frontiers in neuroanatomy},
  year         = {2015},
  volume       = {9},
  pages        = {142},
  abstract     = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic (EM) images of the brain. Participants submitted boundary maps predicted for a test set of images, and were scored based on their agreement with a consensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This "deep learning" approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from cooperation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test dataset. Retrospective evaluation of the challenge scoring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
  completed    = {2015-11-23},
  country      = {Switzerland},
  created      = {2015-11-23},
  doi          = {10.3389/fnana.2015.00142},
  issn-linking = {1662-5129},
  keywords     = {connectomics; electron microscopy; image segmentation; machine learning; reconstruction},
  nlm          = {PMC4633678},
  nlm-id       = {101477943},
  owner        = {NLM},
  pmc          = {PMC4633678},
  pmid         = {26594156},
  pubmodel     = {Electronic-eCollection},
  pubstatus    = {epublish},
  revised      = {2015-12-14},
}

@Article{StoneWildeTaylorEtAl2016,
  author       = {Stone, James R and Wilde, Elisabeth A and Taylor, Brian A and Tate, David F and Levin, Harvey and Bigler, Erin D and Scheibel, Randall S and Newsome, Mary R and Mayer, Andrew R and Abildskov, Tracy and Black, Garrett M and Lennon, Michael J and York, Gerald E and Agarwal, Rajan and DeVillasante, Jorge and Ritter, John L and Walker, Peter B and Ahlers, Stephen T and Tustison, Nicholas J},
  title        = {Supervised learning technique for the automated identification of white matter hyperintensities in traumatic brain injury.},
  journal      = {Brain injury},
  year         = {2016},
  volume       = {30},
  pages        = {1458--1468},
  abstract     = {White matter hyperintensities (WMHs) are foci of abnormal signal intensity in white matter regions seen with magnetic resonance imaging (MRI). WMHs are associated with normal ageing and have shown prognostic value in neurological conditions such as traumatic brain injury (TBI). The impracticality of manually quantifying these lesions limits their clinical utility and motivates the utilization of machine learning techniques for automated segmentation workflows. This study develops a concatenated random forest framework with image features for segmenting WMHs in a TBI cohort. The framework is built upon the Advanced Normalization Tools (ANTs) and ANTsR toolkits. MR (3D FLAIR, T2- and T1-weighted) images from 24 service members and veterans scanned in the Chronic Effects of Neurotrauma Consortium's (CENC) observational study were acquired. Manual annotations were employed for both training and evaluation using a leave-one-out strategy. Performance measures include sensitivity, positive predictive value, [Formula: see text] score and relative volume difference. Final average results were: sensitivity = 0.68 ± 0.38, positive predictive value = 0.51 ± 0.40, [Formula: see text] = 0.52 ± 0.36, relative volume difference = 43 ± 26%. In addition, three lesion size ranges are selected to illustrate the variation in performance with lesion size. Paired with correlative outcome data, supervised learning methods may allow for identification of imaging features predictive of diagnosis and prognosis in individual TBI patients.},
  country      = {England},
  created      = {2016-11-11},
  doi          = {10.1080/02699052.2016.1222080},
  issn         = {1362-301X},
  issn-linking = {0269-9052},
  issue        = {12},
  keywords     = {Neuroimaging; TBI; brain imaging; deep learning; machine learning; magnetic resonance imaging; random forest decision tree},
  nlm-id       = {8710358},
  owner        = {NLM},
  pmid         = {27834541},
  pubmodel     = {Print},
  pubstatus    = {ppublish},
  revised      = {2016-11-12},
}

@Article{ChengNiChouEtAl2016,
  author          = {Cheng, Jie-Zhi and Ni, Dong and Chou, Yi-Hong and Qin, Jing and Tiu, Chui-Mei and Chang, Yeun-Chung and Huang, Chiun-Sheng and Shen, Dinggang and Chen, Chung-Ming},
  title           = {Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in US Images and Pulmonary Nodules in CT Scans.},
  journal         = {Scientific reports},
  year            = {2016},
  volume          = {6},
  pages           = {24454},
  month           = {Apr},
  abstract        = {This paper performs a comprehensive study on the deep-learning-based computer-aided diagnosis (CADx) for the differential diagnosis of benign and malignant nodules/lesions by avoiding the potential errors caused by inaccurate image processing results (e.g., boundary segmentation), as well as the classification bias resulting from a less robust feature set, as involved in most conventional CADx algorithms. Specifically, the stacked denoising auto-encoder (SDAE) is exploited on the two CADx applications for the differentiation of breast ultrasound lesions and lung CT nodules. The SDAE architecture is well equipped with the automatic feature exploration mechanism and noise tolerance advantage, and hence may be suitable to deal with the intrinsically noisy property of medical image data from various imaging modalities. To show the outperformance of SDAE-based CADx over the conventional scheme, two latest conventional CADx algorithms are implemented for comparison. 10 times of 10-fold cross-validations are conducted to illustrate the efficacy of the SDAE-based CADx algorithm. The experimental results show the significant performance boost by the SDAE-based CADx algorithm over the two conventional methods, suggesting that deep learning techniques can potentially change the design paradigm of the CADx systems without the need of explicit design and selection of problem-oriented features.},
  citation-subset = {IM},
  country         = {England},
  created         = {2016-04-15},
  doi             = {10.1038/srep24454},
  issn            = {2045-2322},
  issn-linking    = {2045-2322},
  nlm             = {PMC4832199},
  nlm-id          = {101563288},
  owner           = {NLM},
  pii             = {srep24454},
  pmc             = {PMC4832199},
  pmid            = {27079888},
  pubmodel        = {Electronic},
  pubstatus       = {epublish},
  revised         = {2016-04-20},
}

@Article{HabesErusToledoEtAl2016,
  author          = {Habes, Mohamad and Erus, Guray and Toledo, Jon B and Zhang, Tianhao and Bryan, Nick and Launer, Lenore J and Rosseel, Yves and Janowitz, Deborah and Doshi, Jimit and Van der Auwera, Sandra and von Sarnowski, Bettina and Hegenscheid, Katrin and Hosten, Norbert and Homuth, Georg and Völzke, Henry and Schminke, Ulf and Hoffmann, Wolfgang and Grabe, Hans J and Davatzikos, Christos},
  title           = {White matter hyperintensities and imaging patterns of brain ageing in the general population.},
  journal         = {Brain : a journal of neurology},
  year            = {2016},
  volume          = {139},
  pages           = {1164--1179},
  month           = {Apr},
  abstract        = {White matter hyperintensities are associated with increased risk of dementia and cognitive decline. The current study investigates the relationship between white matter hyperintensities burden and patterns of brain atrophy associated with brain ageing and Alzheimer's disease in a large populatison-based sample (n = 2367) encompassing a wide age range (20-90 years), from the Study of Health in Pomerania. We quantified white matter hyperintensities using automated segmentation and summarized atrophy patterns using machine learning methods resulting in two indices: the SPARE-BA index (capturing age-related brain atrophy), and the SPARE-AD index (previously developed to capture patterns of atrophy found in patients with Alzheimer's disease). A characteristic pattern of age-related accumulation of white matter hyperintensities in both periventricular and deep white matter areas was found. Individuals with high white matter hyperintensities burden showed significantly (P < 0.0001) lower SPARE-BA and higher SPARE-AD values compared to those with low white matter hyperintensities burden, indicating that the former had more patterns of atrophy in brain regions typically affected by ageing and Alzheimer's disease dementia. To investigate a possibly causal role of white matter hyperintensities, structural equation modelling was used to quantify the effect of Framingham cardiovascular disease risk score and white matter hyperintensities burden on SPARE-BA, revealing a statistically significant (P < 0.0001) causal relationship between them. Structural equation modelling showed that the age effect on SPARE-BA was mediated by white matter hyperintensities and cardiovascular risk score each explaining 10.4% and 21.6% of the variance, respectively. The direct age effect explained 70.2% of the SPARE-BA variance. Only white matter hyperintensities significantly mediated the age effect on SPARE-AD explaining 32.8% of the variance. The direct age effect explained 66.0% of the SPARE-AD variance. Multivariable regression showed significant relationship between white matter hyperintensities volume and hypertension (P = 0.001), diabetes mellitus (P = 0.023), smoking (P = 0.002) and education level (P = 0.003). The only significant association with cognitive tests was with the immediate recall of the California verbal and learning memory test. No significant association was present with the APOE genotype. These results support the hypothesis that white matter hyperintensities contribute to patterns of brain atrophy found in beyond-normal brain ageing in the general population. White matter hyperintensities also contribute to brain atrophy patterns in regions related to Alzheimer's disease dementia, in agreement with their known additive role to the likelihood of dementia. Preventive strategies reducing the odds to develop cardiovascular disease and white matter hyperintensities could decrease the incidence or delay the onset of dementia.},
  citation-subset = {AIM, IM},
  completed       = {2016-08-18},
  country         = {England},
  created         = {2016-03-25},
  doi             = {10.1093/brain/aww008},
  issn            = {1460-2156},
  issn-linking    = {0006-8950},
  issue           = {Pt 4},
  keywords        = {Adult; Aged; Aged, 80 and over; Aging, pathology; Alzheimer Disease, diagnosis, epidemiology; Brain, pathology; Cognition Disorders, diagnosis, epidemiology; Cohort Studies; Dementia, diagnosis, epidemiology; Female; Germany, epidemiology; Humans; Magnetic Resonance Imaging, trends; Male; Middle Aged; Poland, epidemiology; Population Surveillance, methods; Risk Factors; White Matter, pathology; Young Adult; Alzheimer’s disease; brain ageing; cardiovascular disease; mild cognitive impairment; white matter hyperintensities},
  nlm             = {PMC5006227 [Available on 04/01/17]},
  nlm-id          = {0372537},
  owner           = {NLM},
  pii             = {aww008},
  pmc             = {PMC5006227},
  pmid            = {26912649},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-12-15},
}

@Article{GuoWuCommanderEtAl2014,
  author          = {Guo, Yanrong and Wu, Guorong and Commander, Leah A and Szary, Stephanie and Jewells, Valerie and Lin, Weili and Shent, Dinggang},
  title           = {Segmenting hippocampus from infant brains by sparse patch matching with deep-learned features.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2014},
  volume          = {17},
  pages           = {308--315},
  abstract        = {Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1- and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the interpatch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods.},
  citation-subset = {IM},
  completed       = {2015-01-08},
  country         = {Germany},
  created         = {2014-12-08},
  issue           = {Pt 2},
  keywords        = {Aging, pathology, physiology; Artificial Intelligence; Hippocampus, anatomy & histology, growth & development; Humans; Image Interpretation, Computer-Assisted, methods; Infant; Infant, Newborn; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  mid             = {NIHMS691875},
  nlm             = {PMC4445142},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmc             = {PMC4445142},
  pmid            = {25485393},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-10-25},
}

@Article{WangDasPlutaEtAl2010,
  author          = {Wang, Hongzhi and Das, Sandhitsu and Pluta, John and Craige, Caryne and Altinay, Murat and Avants, Brian and Weiner, Michael and Mueller, Susanne and Yushkevich, Paul},
  title           = {Standing on the shoulders of giants: improving medical image segmentation via bias correction.},
  journal         = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year            = {2010},
  volume          = {13},
  pages           = {105--112},
  abstract        = {We propose a simple strategy to improve automatic medical image segmentation. The key idea is that without deep understanding of a segmentation method, we can still improve its performance by directly calibrating its results with respect to manual segmentation. We formulate the calibration process as a bias correction problem, which is addressed by machine learning using training data. We apply this methodology on three segmentation problems/methods and show significant improvements for all of them.},
  citation-subset = {IM},
  completed       = {2010-11-15},
  country         = {Germany},
  created         = {2010-09-30},
  issue           = {Pt 3},
  keywords        = {Algorithms; Artifacts; Brain, anatomy & histology; Humans; Image Enhancement, methods; Image Interpretation, Computer-Assisted, methods; Magnetic Resonance Imaging, methods; Pattern Recognition, Automated, methods; Reproducibility of Results; Sensitivity and Specificity},
  mid             = {NIHMS279851},
  nlm             = {PMC3095022},
  nlm-id          = {101249582},
  owner           = {NLM},
  pmc             = {PMC3095022},
  pmid            = {20879389},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-12-08},
}

@Article{AdamsWilson2011,
  author          = {Adams, Christina M and Wilson, Timothy D},
  title           = {Virtual cerebral ventricular system: an MR-based three-dimensional computer model.},
  journal         = {Anatomical sciences education},
  year            = {2011},
  volume          = {4},
  pages           = {340--347},
  abstract        = {The inherent spatial complexity of the human cerebral ventricular system, coupled with its deep position within the brain, poses a problem for conceptualizing its anatomy. Cadaveric dissection, while considered the gold standard of anatomical learning, may be inadequate for learning the anatomy of the cerebral ventricular system; even with intricate dissection, ventricular structures remain difficult to observe. Three-dimensional (3D) computer reconstruction of the ventricular system offers a solution to this problem. This study aims to create an accurate 3D computer reconstruction of the ventricular system with surrounding structures, including the brain and cerebellum, using commercially available 3D rendering software. Magnetic resonance imaging (MRI) scans of a male cadaver were segmented using both semiautomatic and manual tools. Segmentation involves separating voxels of different grayscale values to highlight specific neural structures. User controls enable adding or removing of structures, altering their opacity, and making cross-sectional slices through the model to highlight inner structures. Complex physiologic concepts, such as the flow of cerebrospinal fluid, are also shown using the 3D model of the ventricular system through a video animation. The model can be projected stereoscopically, to increase depth perception and to emphasize spatial relationships between anatomical structures. This model is suited for both self-directed learning and classroom teaching of the 3D anatomical structure and spatial orientation of the ventricles, their connections, and their relation to adjacent neural and skeletal structures.},
  citation-subset = {IM},
  completed       = {2012-03-02},
  country         = {United States},
  created         = {2011-11-08},
  doi             = {10.1002/ase.256},
  issn            = {1935-9780},
  issn-linking    = {1935-9772},
  issue           = {6},
  keywords        = {Aged, 80 and over; Anatomy, education; Cadaver; Cerebral Ventricles, anatomy & histology; Computer Graphics; Computer Simulation; Computer-Assisted Instruction; Humans; Imaging, Three-Dimensional; Learning; Magnetic Resonance Imaging; Male; Models, Anatomic; Software; User-Computer Interface},
  nlm-id          = {101392205},
  owner           = {NLM},
  pmid            = {21976457},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2011-11-08},
}

@Article{McDevittRoweBradyEtAl2014,
  author          = {McDevitt, Elizabeth A and Rowe, Kelly M and Brady, Mark and Duggan, Katherine A and Mednick, Sara C},
  title           = {The benefit of offline sleep and wake for novel object recognition.},
  journal         = {Experimental brain research},
  year            = {2014},
  volume          = {232},
  pages           = {1487--1496},
  month           = {May},
  abstract        = {How do we segment and recognize novel objects? When explicit cues from motion and color are available, object boundary detection is relatively easy. However, under conditions of deep camouflage, in which objects share the same image cues as their background, the visual system must reassign new functional roles to existing image statistics in order to group continuities for detection and segmentation of object boundaries. This bootstrapped learning process is stimulus dependent and requires extensive task-specific training. Using a between-subject design, we tested participants on their ability to segment and recognize novel objects after a consolidation period of sleep or wake. We found a specific role for rapid eye movement (REM, n = 43) sleep in context-invariant novel object learning, and that REM sleep as well as a period of active wake (AW, n = 35) increased segmentation of context-specific object learning compared to a period of quiet wake (QW, n = 38; p = .007 and p = .017, respectively). Performance in the non-REM nap group (n = 32) was not different from the other groups. The REM sleep enhancement effect was especially robust for the top performing quartile of subjects, or "super learners" (p = .037). Together, these results suggest that the construction and generalization of novel representations through bootstrapped learning may benefit from REM sleep, and more specific object learning may also benefit from AW. We discuss these results in the context of shared electrophysiological and neurochemical features of AW and REM sleep, which are distinct from QW and non-REM sleep.},
  citation-subset = {IM},
  completed       = {2014-12-17},
  country         = {Germany},
  created         = {2014-05-06},
  doi             = {10.1007/s00221-014-3830-3},
  issn            = {1432-1106},
  issn-linking    = {0014-4819},
  issue           = {5},
  keywords        = {Adolescent; Adult; Analysis of Variance; Female; Humans; Male; Pattern Recognition, Visual, physiology; Photic Stimulation; Polysomnography; Recognition (Psychology), physiology; Sleep, physiology; Time Factors; Wakefulness, physiology; Young Adult},
  nlm-id          = {0043312},
  owner           = {NLM},
  pmid            = {24504196},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2014-05-06},
}

@Book{vonEcon,
  title     = {Die Cytoarchitektonik der Hirnrinde des erwachsenen Menschen},
  publisher = {Wien: Springer Verlag},
  year      = {1925},
  author    = {Constantin Freiherr von Economo and Georg N Koskinas},
}

@Article{MendrikVinckenKuijfEtAl2015,
  author          = {Mendrik, Adriënne M and Vincken, Koen L and Kuijf, Hugo J and Breeuwer, Marcel and Bouvy, Willem H and de Bresser, Jeroen and Alansary, Amir and de Bruijne, Marleen and Carass, Aaron and El-Baz, Ayman and Jog, Amod and Katyal, Ranveer and Khan, Ali R and van der Lijn, Fedde and Mahmood, Qaiser and Mukherjee, Ryan and van Opbroek, Annegreet and Paneri, Sahil and Pereira, Sérgio and Persson, Mikael and Rajchl, Martin and Sarikaya, Duygu and Smedby, Örjan and Silva, Carlos A and Vrooman, Henri A and Vyas, Saurabh and Wang, Chunliang and Zhao, Liang and Biessels, Geert Jan and Viergever, Max A},
  title           = {MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans.},
  journal         = {Computational intelligence and neuroscience},
  year            = {2015},
  volume          = {2015},
  pages           = {813696},
  abstract        = {Many methods have been proposed for tissue segmentation in brain MRI scans. The multitude of methods proposed complicates the choice of one method above others. We have therefore established the MRBrainS online evaluation framework for evaluating (semi)automatic algorithms that segment gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) on 3T brain MRI scans of elderly subjects (65-80 y). Participants apply their algorithms to the provided data, after which their results are evaluated and ranked. Full manual segmentations of GM, WM, and CSF are available for all scans and used as the reference standard. Five datasets are provided for training and fifteen for testing. The evaluated methods are ranked based on their overall performance to segment GM, WM, and CSF and evaluated using three evaluation metrics (Dice, H95, and AVD) and the results are published on the MRBrainS13 website. We present the results of eleven segmentation algorithms that participated in the MRBrainS13 challenge workshop at MICCAI, where the framework was launched, and three commonly used freeware packages: FreeSurfer, FSL, and SPM. The MRBrainS evaluation framework provides an objective and direct comparison of all evaluated algorithms and can aid in selecting the best performing method for the segmentation goal at hand.},
  citation-subset = {IM},
  completed       = {2016-09-06},
  country         = {United States},
  created         = {2016-01-13},
  doi             = {10.1155/2015/813696},
  issn            = {1687-5273},
  keywords        = {Aged; Aged, 80 and over; Algorithms; Brain, anatomy & histology, physiology; Cerebrospinal Fluid, physiology; Databases, Factual; Female; Gray Matter, anatomy & histology, physiology; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging, methods; Male; Online Systems; Reference Standards; Reproducibility of Results; Software; White Matter, anatomy & histology, physiology},
  nlm             = {PMC4680055},
  nlm-id          = {101279357},
  owner           = {NLM},
  pmc             = {PMC4680055},
  pmid            = {26759553},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{MenzeJakabBauerEtAl2015,
  author          = {Menze, Bjoern H and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-André and Arbel, Tal and Avants, Brian B and Ayache, Nicholas and Buendia, Patricia and Collins, D Louis and Cordier, Nicolas and Corso, Jason J and Criminisi, Antonio and Das, Tilak and Delingette, Hervé and Demiralp, Çağatay and Durst, Christopher R and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M and Jena, Raj and John, Nigel M and Konukoglu, Ender and Lashkari, Danial and Mariz, José Antonió and Meier, Raphael and Pereira, Sérgio and Precup, Doina and Price, Stephen J and Raviv, Tammy Riklin and Reza, Syed M S and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A and Sousa, Nuno and Subbanna, Nagesh K and Szekely, Gabor and Taylor, Thomas J and Thomas, Owen M and Tustison, Nicholas J and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
  title           = {The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS).},
  journal         = {IEEE transactions on medical imaging},
  year            = {2015},
  volume          = {34},
  pages           = {1993--2024},
  month           = {Oct},
  __markedentry   = {[bavants:]},
  abstract        = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  citation-subset = {IM},
  completed       = {2016-07-06},
  country         = {United States},
  created         = {2015-10-07},
  doi             = {10.1109/TMI.2014.2377694},
  issn            = {1558-254X},
  issn-linking    = {0278-0062},
  issue           = {10},
  keywords        = {Algorithms; Benchmarking; Glioma, pathology; Humans; Magnetic Resonance Imaging, methods, standards; Neuroimaging, methods, standards},
  mid             = {NIHMS775317},
  nlm             = {PMC4833122 [Available on 10/01/16]},
  nlm-id          = {8310780},
  owner           = {NLM},
  pmc             = {PMC4833122},
  pmid            = {25494501},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
}

@Article{WangSuhDasEtAl2013,
  author          = {Wang, Hongzhi and Suh, Jung W and Das, Sandhitsu R and Pluta, John B and Craige, Caryne and Yushkevich, Paul A},
  title           = {Multi-Atlas Segmentation with Joint Label Fusion.},
  journal         = {IEEE transactions on pattern analysis and machine intelligence},
  year            = {2013},
  volume          = {35},
  pages           = {611--623},
  month           = {Mar},
  __markedentry   = {[bavants:6]},
  abstract        = {Multi-atlas segmentation is an effective approach for automatically labeling objects of interest in biomedical images. In this approach, multiple expert-segmented example images, called atlases, are registered to a target image, and deformed atlas segmentations are combined using label fusion. Among the proposed label fusion strategies, weighted voting with spatially varying weight distributions derived from atlas-target intensity similarity have been particularly successful. However, one limitation of these strategies is that the weights are computed independently for each atlas, without taking into account the fact that different atlases may produce similar label errors. To address this limitation, we propose a new solution for the label fusion problem in which weighted voting is formulated in terms of minimizing the total expectation of labeling error and in which pairwise dependency between atlases is explicitly modeled as the joint probability of two atlases making a segmentation error at a voxel. This probability is approximated using intensity similarity between a pair of atlases and the target image in the neighborhood of each voxel. We validate our method in two medical image segmentation problems: hippocampus segmentation and hippocampus subfield segmentation in magnetic resonance (MR) images. For both problems, we show consistent and significant improvement over label fusion strategies that assign atlas weights independently.},
  citation-subset = {IM},
  completed       = {2016-05-17},
  country         = {United States},
  created         = {2015-09-10},
  doi             = {10.1109/TPAMI.2012.143},
  issn            = {1939-3539},
  issn-linking    = {0098-5589},
  issue           = {3},
  keywords        = {Algorithms; Databases, Factual; Hippocampus, anatomy & histology; Humans; Image Processing, Computer-Assisted, methods; Magnetic Resonance Imaging},
  mid             = {NIHMS410560},
  nlm             = {PMC3864549},
  nlm-id          = {9885960},
  owner           = {NLM},
  pmc             = {PMC3864549},
  pmid            = {22732662},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2016-10-25},
}

@Comment{jabref-meta: databaseType:bibtex;}
