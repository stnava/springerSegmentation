---
title: Relating high-dimensional structural networks to resting functional connectivity with sparse canonical correlation analysis for neuroimaging
author:
- affiliation: University of Pennsylvania, Philadelphia, PA 19104\footnote{B.A. is
    currently a Biogen employee.}
  name: Brian B. Avants
csl: springerprotocols.csl
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    template: svm-latex-ms.tex
  html_document: default
fontsize: 11pt
geometry: margin=1in
keywords: dimensionality reduction, sparse canonical correlation analysis, sccan,
  eigenanatomy, brain, neuroimaging, resting state, network
link-citations: yes
fontfamily: mathpazo
thanks: This work was supported by K01 ES025432-01
bibliography: cca.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Abstract

<!-- In one or two paragraphs, please write an overview of the method described. -->

Human brain mapping is increasingly faced with the need to relate multiple small sample size, but high-dimensional ("short and wide") datasets.  Exploratory analysis of such data often targets questions regarding network dysfunction in populations for which there is relatively little prior work.  The questions that one may address with such high-dimensional data may relate to traditional "small" measurements such as age, a specific cognitive variable or a risk penotype.  We overview how to use sparse canonical correlation analysis to produce biologically principled low-dimensional representations before proceeding to hypothesis testing, an approach which conserves power by taking advantage of the underlying neurobiological covariation.  We provide an example that maps voxel-wise cortical thickness measurements to resting state network correlations in order to identify structure-function sub-networks with very little further supervision.  The resulting network-like, sparse basis functions allow one to predict traditional univariate outcomes from multiple neuroimaging modalities even when sample sizes are relatively small.


# Introduction

<!-- This section should contain a summary of, and the outline of any theory to, the method that you’re are describing. It should also outline the major procedures involved in the protocol.-->

Sparse canonical correlation analysis is gaining popularity within biomedical analysis as a tool for handling multiview high-dimensional datasets [@WittenTibshiraniHastie2009; @AvantsCookUngarEtAl2010; @AvantsCookMcMillanEtAl2010; @ChaliseBatzlerAboEtAl2012; @DudaDetreKimEtAl2013; @LinCalhounWang2014; @AvantsLibonRascovskyEtAl2014; @FangLinSchulzEtAl2016; @DuHuangYanEtAl2016].  Perhaps the first effort to extend Hotelling's original work [@hotelling1936relations] to sparse solutions involved relating specific words within musical annotations directly to specific sounds [@torres2007finding] by adding $\ell_1$ regularization. More recently, the method is gaining traction in imaging genomics to relate imaging phenotypes to genotype, potentially by taking advantage of prior knowledge in the form of structured sparsity [@DuHuangYanEtAl2016]. 

Despite these new applications, the advantages of CCA remain the same today as in 1936: CCA allows one to symmetrically relate sets of variables to each other.  Specifically, consider two matrices, $X$ (dimension $n \times p$ ) and $Y$ (dimension $n \times q$).  If we want a direct univariate comparison between the columns of these matrices, then we would have to perform on the order of $p \times q$ pairwise calculations.  Clearly, if $p$ and $q$ are large (e.g. greater than several thousand), then this results in a very large computational burden.  Becausing neuroimaging information is often highly redundant, this univariate comparison forces many more individual comparisons than should be necessary to gain an understanding of the robustness of the relationship between these measurements.  In contrast, CCA can make this comparison with a single multivariate calculation that maximizes:
$$
x^\star, y^\star = argmax_{x,y} ~~Corr( X u, Y v ) 
$$
where Corr represents Pearson correlation and $x$ and $y$ are the solution vectors (or matrices if we are computing multiple canonical variates.)  The variables $x$ and $y$ project $X$ and $Y$ into two $n \times 1$ vectors.  While it is clear that the directly multivariate approach gains power by avoiding multiple comparisons, one compromises interpretability.  That is, the factors that traditional CCA produces are dense.  In analogy to linear regression, the "$\beta$ weights" are non-zero on both sides of the equation.  

The need to increase interpretability motivates sparse CCA, which allows $x$ and $y$ to have entries which are "mostly zero" and therefore have more specificity.  This is a useful property in neuroscience which seeks to understand the hierarchical and (somewhat) segregated nature of the brain (and focal impact of brain disorders).  Sparse CCA allows one to powerfully address the question of how patterns of variability in cortical structure relate to patterns of resting activity, as measured by functional magnetic resonance imaging. This question is fundamental to understanding not only how structure recapitulates function but also how changes in the organization of the brain may precede the onset of clinical symptoms.  Relatively few studies address the question of how functional connectivity directly associates with cortical (or structural) networks [@Romero-GarciaAtienzaCantero2014; @MarstallerWilliamsRichEtAl2015] where we define a structural network as a set of covarying cortical regions [@KhundrakpamLewisReidEtAl2017].  The relative paucity of studies may relate to the lack of freely available statistical frameworks for addressing this question.

The ability to project large sets of complementary neuroimaging measurements into a low-dimensional and interpretable space is a powerful one for brain mapping. The CCA (and sparse CCA) methodology permits hypotheses about the brain to be framed in new and fundamentally multivariate ways.  As we will see below, with proper constraints, sparse CCA can produce solutions that in some sense capture what we think of as networks: sets of measurements that covary, presumably due to underlying neurobiological mechanisms.  This mitigates several problems:  multiple comparisons correction, perhaps too rigid prior definitions of network characteristics and the need to "translate" artificially independent voxel-based statistical tests into regularized statistical maps (by, for example, cluster constrained post-processing).  Below, we will make these statements specific by employing a public dataset to show how we can employ sparse canonical correlation analysis for neuroimaging (SCCAN) [@AvantsLibonRascovskyEtAl2014] to compute a structure-function basis set by associating whole brain cortical thickness  directly to resting state network correlation matrices.  We use SCCAN to infer the structural patterns that relate to functional connectivity patterns and how these may be used to build an interpretable, predictive regression model (based on multiple neuroimaging modalities) for demographic variables.  We detail elements of the software, visualization, interpretation and  evaluation with reference to the clinical considerations raised by the freely available Pre-symptomatic Evaluation of Novel or Experimental Treatments for Alzheimer's Disease (PREVENT-AD) dataset [@OrbanMadjarSavardEtAl2015] on which we focus.


# Materials


## Imaging data

We demonstrate our application on the T1-weighted and resting state fMRI neuroimages collected in the PREVENT-AD study described in detail in [@OrbanMadjarSavardEtAl2015]. The cohort subjects are cognitively normal with average age 65.4 $\pm$ 6.3 years (min 55.0, max 84.0); PREVENT-AD includes 58 females and 22 males with documented family history of Alzheimer's disease (AD) but with good general health at enrollment.  Subjects will be followed longitudinally to quantify the difference in normal aging and progression to AD in this high-risk group. All 80 subjects are scanned at two different sessions conducted within 111.4 $\pm$ 24.3 days of each other.  Each session includes 1mm$^3$ T1-weighted MRI and resting state functional MRI collected on a 3T Siemens machine as described [@OrbanMadjarSavardEtAl2015].  The resting state fMRI comprises two 5 min 45 second runs within each session.  Due to the relative temporal proximity of the sessions and collection of multiple runs within each session, this dataset presents a valuable opportunity to assess fMRI reliability in this special cohort, as covered in the data source publication.  However, we focus only on the first session data which precedes randomized treatment by Naproxen that could potentially impact the second session data.  

### Structural image processing 

We processed each T1 image through the ANTs cortical thickness pipeline [@TustisonCookKleinEtAl2014].  After viewing several example datasets from the PREVENT-AD cohort, we selected the "Oasis" template from [@TustisonCookKleinEtAl2014] due to its similarity in terms of T1 contrast and defacing protocol to PREVENT-AD. ANTs cortical thickness produces a six tissue segmentation which has been validated with respect to FreeSurfer [@TustisonCookKleinEtAl2014] and other segmentation methods [@AvantsTustisonWuEtAl2011].  This segmentation procedure performs brain extraction followed by tissue segmentation within the brain mask.  The brain masking algorithm involves bias correction, two registration steps, segmentation and morphological operations.  The probability maps that emerge from this part of the pipeline are passed to a volumetric, voxel-wise cortical thickness measurement algorithm, DiReCT [@DasAvantsGrossmanEtAl2009].  DiReCT accounts for buried sulci while incorporating tissue-class uncertainties in the thickness estimation step and provides a volumetric alternative that may extract more meaningful variability in comparison to Freesurfer in some cohorts [@TustisonCookKleinEtAl2014].  The thickness data for each subject is transferred to the structural T1 template space.  By masking the thickness data with a group-wise cortical mask, we transform the voxel representation to a matrix represenation. Below, the thickness data will comprise the "left-hand" view in SCCAN, i.e. the $X$ matrix.

### Functional image processing 

We organize our functional image processing strategy around the Power coordinate system [@PowerCohenNelsonEtAl2011] which we will use to generate connectivity matrices of size $270 \times 270$.  The additional two nodes correspond to the left and right head of the hippocampus. Such matrices allow a compressed representation of whole-brain connectivity patterns [@ShirerRyaliRykhlevskaiaEtAl2012] indexed at reliable anatomical coordinates derived from meta-analysis in task studies. We motion correct each run for the baseline session in the PREVENT-AD cohort to a subject-specific, iteratively computed times series mean and collect the final transformation parameters for later analysis as part of quality assurance.  We also compute nuisance variables within non-cortical tissue via CompCor, keeping 10 nuisance predictors as in [@ShirerJiangPriceEtAl2015].  We residualize nuisance variables from the BOLD signal within the cerebrum and compute the pairwise correlation between all Power nodes. We subsequently align the mean resting BOLD image to the individual time point brain extracted T1.  We then map the resting BOLD data to the group space through a composite transformation. Finally, we generate a vector representation of the connection matrix from the upper triangle.  This will, below, enter into $Y$, i.e. the "right hand" matrix in SCCAN.  A subset of individual timepoints of resting state exhibited excessive motion artifact, based on framewise displacement, and were removed from further analysis ($n=$8).  We briefly investigated the reliability of this processing stream and found it to be consistent with results reported in [@OrbanMadjarSavardEtAl2015].



## Software

We employ R version 3.3.1 ("Bug in Your Hair") for basic statistical processing. We also employ ANTsR version 0.3.3 [@TustisonShrinidhiWintermarkEtAl2015] for core image analysis, data organization and early development efforts.  ANTsR, itself, links to ITKR, ANTs and the ITK toolkits.  The specific versions of each concomitant library is documented within ANTsR software.  However, we note that the only "pure ANTs" tool used in this analysis is well-validated ants cortical thickness pipeline as above.  The remainder of the processing is performed within ANTsR which is available both at [http://stnava.github.io/ANTsR/](http://stnava.github.io/ANTsR/) and via Neuroconductor [https://neuroconductor.org/](https://neuroconductor.org/).  In addition to the PREVENT-AD neuroimaging, experiments in this paper additionally require the `boot` R package.  ANTsR, itself, has several dependencies which are identified at its webpage.

### Toy dataset

We use, as example, a normalized ( registed to common template space ) structural population dataset available in ANTsR.  By normalizing these brain slices to a common template, we can generate intuition about SCCAN covariance patterns via fast and easy to reproduce examples.  We first define the six subjects and use the first as template.
```{r exampler0,echo=TRUE,eval=TRUE}
library( ANTsR )
fns = c( "r16", "r27", "r30", "r62", "r64", "r85" )
ref = antsImageRead( getANTsRData( fns[1] ) )
```
\noindent We then deformably register the group to the template and define a brain mask.  We also compute the jacobian determinant and the deformed gradient images to fill in toy $X$ and $Y$ matrices for later use.
```{r exampler,echo=TRUE,eval=FALSE}
mask = getMask( ref )
X = matrix( nrow = length( fns ), ncol = sum( mask ) )
Y = matrix( nrow = length( fns ), ncol = sum( mask ) )
for ( i in 1:length( fns ) )
  {
  tar = antsImageRead(  getANTsRData( fns[i] )  )
  reg = antsRegistration( ref, tar, typeofTransform = 'SyN' )
  jac = createJacobianDeterminantImage( ref, reg$fwdtransforms[1], 1 )
  X[ i, ] = jac[ mask == 1 ]
  tgr = antsApplyTransforms( ref, iMath( tar, "Grad", 1 ), reg$fwdtransforms )
  Y[ i, ] = smoothImage( tgr, 1.5 )[ mask == 1 ]
}
```



# Methods

This is the main section and should explain in detail the individual steps necessary to carry out the technique. Where possible, please simply list the steps in numerical order. For techniques that comprise a number of separate major procedures, please indicate these separate procedures in the introduction, and then subdivide section 3 into subheadings to cover each procedure. The steps in each subsection should then be numbered individually, renumbering 2 from number one. Do take great care to try to indicate any little "tricks" or nuances that help improve your method by referring to relevant "notes" in section 4 (see below). This sort of information rarely gets into the scientific literature. You may also find it useful to relate to some aspects of the theory in this section indicating the purpose of some of the major steps by crossreferencing to an appropriate “note”. Do not be tempted to get involved in the description of variations/alternatives to your technique in this section: this can be done in the "Notes" section. Stick to the basic procedure detailed in this section.

This section must be comprehensive. Do not send the reader away to find information for a particular step in another reference. All relevant practical detail must be given in this section.


Sparse canonical correlation analysis for neuroimaging (SCCAN) is a general purpose
tool for "two-sided" multiple regression [@AvantsLibonRascovskyEtAl2014] that, along with other variants of CCA, can address such questions.  SCCAN, available within ANTsR [@AvantsDudaKilroyEtAl2015], exploits covariation across large datasets while imposing dataset-specific spatial regularization that helps prevent overfitting, especially important when $p >> n$ i.e. when the number of data measurements is much greater than the number of subjects on which these are measured.  SCCAN allows one to symmetrically compare one matrix of data to another and find linear relationships between them in a low-dimensional space, just like CCA, but with additional regularization parameters:
$$
x^\star, y^\star = argmax_{x,y} ~~\frac{x X^T~Y y}{\|Xx\|\|Yy\|}  - \gamma_x \| G^t_{\sigma x} \star x \|^+_1 - \gamma_y \| G^t_{\sigma y} \star y \|^+_1.
$$
Here, $\gamma_\cdot$ is a scalar weighting term for each variate that controls the impact of the $\ell_1^+$ penalty on the objective function (here denoted $\| \cdot \|^+_1$ ). The $\ell_1^+$ penalty differs from $\ell_1$ in that it sends negative values to zero.  The operator $G^t_{\sigma \cdot}$ performs spatial regularization (usually Gaussian or a modified Gaussian) on the solution vectors $x$ or $y$ where the amount of regularization is determined by $\sigma \cdot$. Much like singular value decomposition (SVD), the resulting subspace encodes a basis that may be used to reconstruct the original dataset. Also, like SVD, multiple solution vectors can be obtained from the above optimization which can be done in parallel or via deflation [@WittenTibshiraniHastie2009]. In comparison to methods which employ standard $\ell_1$ regularization, SCCAN has the benefit of inserting the spatial regularization inside the operator and projecting solutions to the non-negative space.  This means that the resulting solution vectors can be interpreted within the original units of the data i.e. similar to regions of interest.


## Example 2D data

```{r antsrex1,echo=FALSE,eval=TRUE}
library( ANTsR )
img = antsImageRead( getANTsRData( "r16" ) )
msk = getMask( img )
r = 5
myk = 50
mat = getNeighborhoodInMask( img, msk, rep( r, img@dimension ), boundary.condition = 'mean' )
```

Given a matrix representation of image neighborhood structure, we can express the relationship
between ...


Here is the code:
```{r ex1,echo=FALSE,eval=FALSE}
x = scale( mat, scale=F )
myk = 100
kcov = sparseDecom2( x, k=myk, kmetric = 'cov', mypkg = 'rflann' )
```

Look at a subset of the sparse matrix.

```{r viewsmat,echo=FALSE,eval=FALSE}
image( kcov[1:500,1:500] )
```

zooming in shows local clusters in the sparse matrix:

```{r viewsmat2,echo=FALSE,eval=FALSE}
inds = 20:80
image( kcov[inds,inds] )
```


We compute the eigenvectors of this matrix using the IRLBA -- the augmented implicitly restarted Lanczos bidiagonalization algorithm.  IRLBA finds (few) approximate largest singular values and corresponding singular vectors of a sparse or dense matrix and is both a fast and memory-efficient way to compute a partial SVD.

```{r rirl,echo=FALSE,eval=FALSE}
mysvd = irlba( kcov, nv = 500 )
print( dim( mysvd$v ) )
```

Turn the eigenvector back into an image.

```{r vimg,echo=FALSE,eval=FALSE}
overlaymin = 0.1
mimg1 = makeImage( mask, abs(mysvd$v[,1] ) ) %>% iMath("Normalize")
mimg2 = makeImage( mask, abs(mysvd$v[,2] ) ) %>% iMath("Normalize")
plot( ref, mimg1, window.overlay=c(overlaymin,1) )
plot( ref, mimg2, window.overlay=c(overlaymin,1) )
```

Note that we display the top 90\% of the eigenvector values.  The majority of the eigenvector is sparse.  This sparsity is controlled by the value of $k$ which, here, is `r myk`.

Let us look how this changes over values of $k$: Show $k=5$.
```{r ksrch5,echo=FALSE,eval=FALSE}
myk2 = 5
print( paste( "k-value", myk2 ) )
kcov = sparseDistanceMatrix( x, k=myk2, kmetric = 'cov', mypkg = 'rflann' )
mysvd2 = irlba( kcov, nv = 2 )
mimg2 = makeImage( mask, abs(mysvd2$v[,2] ) ) %>% iMath("Normalize")
plot( ref, mimg2, window.overlay=c(overlaymin,1) )
```



Note the rapid initial decay followed by a slow falloff --- also note that this covariance matrix is based on `r length(fns)` subjects.  This suggests that we are effectively investigating local submanifolds of covariance patterns vs the global linear modeling provided by the classic SVD (which would limit the number of eigenvectors we can compute to `r length(fns)-1`).

Finally, let us compare to "classic" SVD wherein -- using the same methods -- we see that a dense eigenvector results.

```{r oldsvd,echo=FALSE,eval=FALSE}
oldsvd = svd( x, nu = 0, nv = 2 )
mimg2 = makeImage( mask, abs(oldsvd$v[,2] ) ) %>% iMath("Normalize")
plot( ref, mimg2, window.overlay=c(overlaymin,1) )
```


## Structural networks and resting connectivity in PREVENT-AD

Age and cognition impact structural covariance [@FranzmeierBuergerTeipelEtAl2017] as well as resting connectivity [@FranzmeierBuergerTeipelEtAl2017].  If structural covariance also covaries with resting connectivity, then we can exploit SCCAN to identify covarying sub-networks from both types of data that may --- in downstream hypothesis testing --- relate to age, cognition or, potentially, disease status.  We follow a "clustering before hypothesis testing" approach similar to that proposed in eigenanatomy [@] but where the clustering of the data is based on covariation across modalities rather than within a single modality.  

In summary, computing $knn$-sparse covariance matrices enables a natural and fast method for investigating the principal eigenvectors derived from multiscale covariance structure.  In particular, the eigenvectors of such matrices appear to be sparse with non-zero content that increases with $k$.  This leads to a new approach to computing sparse eigenvectors that takes advantage of existing methods for computing fast SVD of sparse matrices and reduces the need for specific optimization algorithms.

We can construct a matrix that is very similar to the $knn$-covariance matrix but includes an additional constraint of physical proximity.  In this case, we can argue that, by construction, the $knn$-spatial-covariance matrix is a sum of sparse and low-rank matrices.  For instance, one element in this sum would be defined by the covariance of the voxels centered at $x_m$ with some given spatial neighborhood radius $r$.

```{r spatialconstraint,echo=FALSE,eval=FALSE}
spatmat = t( imageDomainToSpatialMatrix( mask, mask ) )
spatrad = sparseDistanceMatrix( spatmat, k = 500, kmetric = "gaussian",
                                mypkg = 'rflann', sigma = 50.0 )
```

Visualize this spatial matrix.

```{r spatmat,echo=FALSE,eval=FALSE}
inds = 500:1000
image( spatrad[ inds, inds ] )
```

zoom in more

```{r spatmat2,echo=FALSE,eval=FALSE}
inds = 500:600
image( spatrad[ inds, inds ] )
```


# Notes

## Note 1

Scaling the matrix inputs is a critical decision.  

* No scaling

```{r nosc}
```


* Centering:

```{r conly}
```

* Centering and scaling:

```{r cands}
```

Each case ...

## Note 2

investigate i.e. visualize the candidate hypotheses.  in our example above, the research might treat the CCA output as "suggested hypotheses."  Candidate solutions may be rejected before passing them down to the stage wherein they are tested against variables that will compromise power (age in the example above).

## Note 3

Permutation is, at times, recommended for evaluating significance [@WittenTibshiraniHastie2009].  However, in our experience this may be anti-conservative in $p >> n$ datasets.  An effective alternative is to employ cross-validation of the correlation with permutation.  That is, rather than asking "how well do these correlation hold up under permutation?", one might ask "how does cross-validation performance change between real and permuted data"?  This latter strategy may be more informative and stable in smaller datasets.

## Note 4

Robust data transformations can address the sensitivity of correlation-based analyses to outliers [@WilmsCroux2016].  ANTsR provides a function `robustMatrixTransform` which transforms each data-point by a rank transform and thus acts as a general purpose and easy to use safeguard against such issues (although we note that careful data inspection is always a superior approach).  Referring back to our example dataset, we can see how this option can be used:


```{r myrob}
```

# References